{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67525bb4-7491-4028-a288-6f8fb3fbf6ee",
   "metadata": {},
   "source": [
    "# Evaluate NER performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40165e0d-85e9-43e7-9450-8bd308ad1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete everything about occupation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d421b-012d-454c-ae1e-7da6cc13e4a7",
   "metadata": {},
   "source": [
    "## TODO :\n",
    "\n",
    "- faire le groundtruth dataset\n",
    "- faire textually close dataset\n",
    "- faire le lax close dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60ff5f78-7dc8-405d-9bc0-249b9970993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../src')\n",
    "\n",
    "# from character_extraction import *\n",
    "from embeddings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "967ed151-f492-4a91-8da5-547462321b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clustering_metrics(embeddings, embeddings_type):\n",
    "    '''Given embeddings, and their ground truth data type, computes several clustering performance\n",
    "    metrics. The right `ground_truth_data_df`, `textually_close_ent_ground_truth_df` or \n",
    "    `lax_ent_ground_truth_df` should have been loaded into memory before calling this function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings : dictionary\n",
    "        The dictionary containing each entity and their associated embedding vector\n",
    "    embeddings_type : str\n",
    "        The matching ground truth data type for the given embeddings (either 'first_version',\n",
    "        'textually_close' or 'lax')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    same_entityness : list\n",
    "        A list containing the performance metrics with regards to the 'same_entityness' axis\n",
    "    gender : list\n",
    "        A list containing the performance metrics with regards to the 'gender' axis\n",
    "    first_appearance : list\n",
    "        A list containing the performance metrics with regards to the 'first_appearance' axis\n",
    "    '''\n",
    "    \n",
    "    # SAME ENTITY-NESS\n",
    "    same_entityness = []\n",
    "    \n",
    "    if embeddings_type == 'first_version':\n",
    "        mask_embs_entity = [(k, \n",
    "                             embeddings[k], \n",
    "                             ground_truth_data_df[ground_truth_data_df['name'] == k]['entity_ID'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k.lower() in ground_truth_data_df['name'].tolist()]\n",
    "    elif embeddings_type == 'textually_close':\n",
    "        mask_embs_entity = [(k, \n",
    "                             embeddings[k]['MASK'], \n",
    "                             textually_close_ent_ground_truth_df[textually_close_ent_ground_truth_df['name'] == k]['entity_ID'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k in textually_close_ent_ground_truth_df['name'].tolist()]\n",
    "    elif embeddings_type == 'lax':\n",
    "        mask_embs_entity = [(k, \n",
    "                             embeddings[k]['MASK'], \n",
    "                             lax_ent_ground_truth_df[lax_ent_ground_truth_df['name'] == k]['entity_ID'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k in lax_ent_ground_truth_df['name'].tolist()]\n",
    "        \n",
    "    tmp_df = pd.DataFrame(mask_embs_entity)\n",
    "    same_entityness.append(sklearn.metrics.silhouette_score(np.array(tmp_df[1].tolist()), \n",
    "                                                            np.array(tmp_df[2]), \n",
    "                                                            metric='euclidean', \n",
    "                                                            random_state=0))\n",
    "    \n",
    "    same_entityness.append(sklearn.metrics.calinski_harabasz_score(np.array(tmp_df[1].tolist()), \n",
    "                                                                   np.array(tmp_df[2])))\n",
    "    \n",
    "    same_entityness.append(sklearn.metrics.davies_bouldin_score(np.array(tmp_df[1].tolist()), \n",
    "                                                                np.array(tmp_df[2])))\n",
    "    \n",
    "    tmp_df = pd.DataFrame(mask_embs_entity)\n",
    "    entityness_matrix = np.array([np.array(emb) for emb in tmp_df[1]])\n",
    "    k_choice = 45 # obtained by the elbow method\n",
    "    kmean = KMeans(n_clusters=k_choice, random_state=0).fit(entityness_matrix, )\n",
    "    predicted_clusters = kmean.predict(np.array([np.array(emb) for emb in tmp_df[1]]))\n",
    "    \n",
    "    same_entityness.append(sklearn.metrics.rand_score(np.array(tmp_df[2]), predicted_clusters))\n",
    "    same_entityness.append(sklearn.metrics.adjusted_rand_score(np.array(tmp_df[2]), predicted_clusters))\n",
    "    same_entityness.append(sklearn.metrics.mutual_info_score(np.array(tmp_df[2]), predicted_clusters))\n",
    "    same_entityness.append(sklearn.metrics.adjusted_mutual_info_score(np.array(tmp_df[2]), \n",
    "                                                                      predicted_clusters, \n",
    "                                                                      average_method='arithmetic'))\n",
    "    \n",
    "    \n",
    "    # GENDER\n",
    "    gender = []\n",
    "    \n",
    "    if embeddings_type == 'first_version':\n",
    "        mask_embs_gender = [(k, \n",
    "                             embeddings[k], \n",
    "                             ground_truth_data_df[ground_truth_data_df['name'] == k]['gender'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k.lower() in ground_truth_data_df['name'].tolist()]\n",
    "    elif embeddings_type == 'textually_close':\n",
    "        mask_embs_gender = [(k, \n",
    "                             embeddings[k]['MASK'], \n",
    "                             textually_close_ent_ground_truth_df[textually_close_ent_ground_truth_df['name'] == k]['gender'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k in textually_close_ent_ground_truth_df['name'].tolist()]\n",
    "    elif embeddings_type == 'lax':\n",
    "        mask_embs_gender = [(k, \n",
    "                             embeddings[k]['MASK'], \n",
    "                             lax_ent_ground_truth_df[lax_ent_ground_truth_df['name'] == k]['gender'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k in lax_ent_ground_truth_df['name'].tolist()]\n",
    "\n",
    "    tmp_df = pd.DataFrame(mask_embs_gender)\n",
    "    gender.append(sklearn.metrics.silhouette_score(np.array(tmp_df[1].tolist()), \n",
    "                                                   np.array(tmp_df[2] == 'M').astype(int), \n",
    "                                                   metric='euclidean', \n",
    "                                                   random_state=0))\n",
    "    gender.append(sklearn.metrics.calinski_harabasz_score(np.array(tmp_df[1].tolist()), np.array(tmp_df[2])))\n",
    "    gender.append(sklearn.metrics.davies_bouldin_score(np.array(tmp_df[1].tolist()), np.array(tmp_df[2])))\n",
    "    \n",
    "    tmp_df = pd.DataFrame(mask_embs_gender)\n",
    "    gender_matrix = np.array([np.array(emb) for emb in tmp_df[1]])\n",
    "    k_choice = 2 # two genders in PG literature (men and women)\n",
    "    kmean = KMeans(n_clusters=k_choice, random_state=0).fit(gender_matrix)\n",
    "    predicted_clusters = kmean.predict(np.array([np.array(emb) for emb in tmp_df[1]]))\n",
    "    \n",
    "    gender.append(sklearn.metrics.rand_score(np.array(tmp_df[2]), predicted_clusters))\n",
    "    gender.append(sklearn.metrics.adjusted_rand_score(np.array(tmp_df[2]), predicted_clusters))\n",
    "    gender.append(sklearn.metrics.mutual_info_score(np.array(tmp_df[2]), predicted_clusters))\n",
    "    gender.append(sklearn.metrics.adjusted_mutual_info_score(np.array(tmp_df[2]), predicted_clusters, \n",
    "                                                             average_method='arithmetic'))\n",
    "    \n",
    "    # FIRST APPEARANCE\n",
    "    first_appearance = []\n",
    "    \n",
    "    # build distance matrix \n",
    "    if embeddings_type == 'first_version':\n",
    "        mask_embs_appear = [(k, \n",
    "                             embeddings[k], \n",
    "                             ground_truth_data_df[ground_truth_data_df['name'] == k]['first appearance'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k.lower() in ground_truth_data_df['name'].tolist()]\n",
    "    elif embeddings_type == 'textually_close':\n",
    "        mask_embs_appear = [(k, \n",
    "                             embeddings[k]['MASK'], \n",
    "                             textually_close_ent_ground_truth_df[textually_close_ent_ground_truth_df['name'] == k]['first appearance'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k in textually_close_ent_ground_truth_df['name'].tolist()]\n",
    "    elif embeddings_type == 'lax':\n",
    "        mask_embs_appear = [(k, \n",
    "                             embeddings[k]['MASK'], \n",
    "                             lax_ent_ground_truth_df[lax_ent_ground_truth_df['name'] == k]['first appearance'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k in lax_ent_ground_truth_df['name'].tolist()]\n",
    "        \n",
    "    tmp_df = pd.DataFrame(mask_embs_appear)\n",
    "    appear_matrix = np.array(tmp_df[2]).reshape(-1, 1)\n",
    "\n",
    "    # k based both on \"vector\" being predict (first appearance in book) and overall clustering\n",
    "    # using elbow shape\n",
    "    k_choice = 17\n",
    "    kmean = KMeans(n_clusters=k_choice, random_state=0).fit(appear_matrix)\n",
    "\n",
    "    first_appearance.append(sklearn.metrics.silhouette_score(np.array(tmp_df[1].tolist()), \n",
    "                                         kmean.predict(np.array(tmp_df[2]).reshape(-1,1)), \n",
    "                                         metric='euclidean', \n",
    "                                         random_state=0))\n",
    "    \n",
    "    first_appearance.append(sklearn.metrics.calinski_harabasz_score(np.array(tmp_df[1].tolist()), \n",
    "                                 kmean.predict(np.array(tmp_df[2]).reshape(-1,1))))\n",
    "    \n",
    "    first_appearance.append(sklearn.metrics.davies_bouldin_score(np.array(tmp_df[1].tolist()), \n",
    "                                 kmean.predict(np.array(tmp_df[2]).reshape(-1,1))))\n",
    "    \n",
    "    tmp_df = pd.DataFrame(mask_embs_appear)\n",
    "    ground_truth_based_clusters = kmean.predict(np.array(tmp_df[2]).reshape(-1,1))\n",
    "    appear_matrix = np.array([np.array(emb) for emb in tmp_df[1]])\n",
    "    kmean = KMeans(n_clusters=k_choice, random_state=0).fit(appear_matrix)\n",
    "    predicted_clusters = kmean.predict(np.array([np.array(emb) for emb in tmp_df[1]]))\n",
    "    \n",
    "    first_appearance.append(sklearn.metrics.rand_score(ground_truth_based_clusters, predicted_clusters))\n",
    "    first_appearance.append(sklearn.metrics.adjusted_rand_score(ground_truth_based_clusters, predicted_clusters))\n",
    "    first_appearance.append(sklearn.metrics.mutual_info_score(ground_truth_based_clusters, predicted_clusters))\n",
    "    first_appearance.append(sklearn.metrics.adjusted_mutual_info_score(ground_truth_based_clusters, predicted_clusters, \n",
    "                                                                       average_method='arithmetic'))\n",
    "    \n",
    "    return same_entityness, gender, first_appearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b86dbd94-1ab5-4bbe-91b4-41dcb7f8009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clustering_metrics(embeddings, embeddings_type):\n",
    "    '''Given embeddings, and their ground truth data type, display in a table several\n",
    "    clustering performance metrics. The right `ground_truth_data_df`, \n",
    "    `textually_close_ent_ground_truth_df` or `lax_ent_ground_truth_df` should have been \n",
    "    loaded into memory before calling this function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings : dictionary\n",
    "        The dictionary containing each entity and their associated embedding vector\n",
    "    embeddings_type : str\n",
    "        The matching ground truth data type for the given embeddings (either 'first_version',\n",
    "        'textually_close' or 'lax')\n",
    "    '''\n",
    "    \n",
    "    same_entityness, gender, first_appearance = get_clustering_metrics(embeddings, embeddings_type)\n",
    "    print('--------------------------------------------------------------------------------------------')\n",
    "    print('|                            | Same Entity-ness |  Gender  | First Appearance |')\n",
    "    print('--------------------------------------------------------------------------------------------')\n",
    "    print(f'| Silhouette Score           |     {same_entityness[0]:8.5f}     | {gender[0]:8.5f} |  {first_appearance[0]:8.5f}     |')\n",
    "    print(f'| Calinski Harabasz Score    |     {same_entityness[1]:8.5f}     | {gender[1]:8.5f} |  {first_appearance[1]:8.5f}     |')\n",
    "    print(f'| Davies Bouldin Score       |     {same_entityness[2]:8.5f}     | {gender[2]:8.5f} |  {first_appearance[2]:8.5f}     |')\n",
    "    print(f'| Rand Score                 |     {same_entityness[3]:8.5f}     | {gender[3]:8.5f} |  {first_appearance[3]:8.5f}     |')\n",
    "    print(f'| Adjusted Rand Score        |     {same_entityness[4]:8.5f}     | {gender[4]:8.5f} |  {first_appearance[4]:8.5f}     |')\n",
    "    print(f'| Mutual Info Score          |     {same_entityness[5]:8.5f}     | {gender[5]:8.5f} |  {first_appearance[5]:8.5f}     |')\n",
    "    print(f'| Adjusted Mutual Info Score |     {same_entityness[6]:8.5f}     | {gender[6]:8.5f} |  {first_appearance[6]:8.5f}     |')\n",
    "    print('--------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca813ee6-8b9e-48ae-8d27-5f4dc4a4699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_entityness = []\n",
    "    \n",
    "if embeddings_type == 'first_version':\n",
    "    mask_embs_entity = [(k, \n",
    "                         embeddings[k], \n",
    "                         ground_truth_data_df[ground_truth_data_df['name'] == k]['entity_ID'].values[0]) \n",
    "                        for k in embeddings \n",
    "                        if k.lower() in ground_truth_data_df['name'].tolist()]\n",
    "elif embeddings_type == 'textually_close':\n",
    "    mask_embs_entity = [(k, \n",
    "                         embeddings[k]['MASK'], \n",
    "                         textually_close_ent_ground_truth_df[textually_close_ent_ground_truth_df['name'] == k]['entity_ID'].values[0]) \n",
    "                        for k in embeddings \n",
    "                        if k in textually_close_ent_ground_truth_df['name'].tolist()]\n",
    "elif embeddings_type == 'lax':\n",
    "    mask_embs_entity = [(k, \n",
    "                         embeddings[k]['MASK'], \n",
    "                         lax_ent_ground_truth_df[lax_ent_ground_truth_df['name'] == k]['entity_ID'].values[0]) \n",
    "                        for k in embeddings \n",
    "                        if k in lax_ent_ground_truth_df['name'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa4e9982-3568-4f2b-a461-568d6f935ace",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at flaubert/flaubert_base_cased were not used when initializing FlaubertModel: ['pred_layer.proj.weight', 'pred_layer.proj.bias']\n",
      "- This IS expected if you are initializing FlaubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|▏                                                                                                                    | 13/7929 [00:32<5:28:00,  2.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3962115/1994861408.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeddings_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrench_word_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'flaubert/flaubert_base_cased'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'798-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/students/doyard/sentiment-evolution-in-characters-network/scripts/../src/embeddings.py\u001b[0m in \u001b[0;36mfrench_word_embeddings\u001b[0;34m(model_name, gutenberg_id)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprocessed_sentenced\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflaubert_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_sentenced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflaubert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_sentenced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/students/doyard/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/students/doyard/lib/python3.8/site-packages/transformers/models/flaubert/modeling_flaubert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, langs, token_type_ids, position_ids, lengths, cache, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;31m# self attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_norm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 attn_outputs = self.attentions[i](\n\u001b[0m\u001b[1;32m    264\u001b[0m                     \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                     \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/students/doyard/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/students/doyard/lib/python3.8/site-packages/transformers/models/xlm/modeling_xlm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, mask, kv, cache, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdim_per_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_lin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, qlen, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_lin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, qlen, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/students/doyard/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/students/doyard/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddings_dict = french_word_embeddings('flaubert/flaubert_base_cased', '798-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f2f09-ceee-4adc-9a68-f428b6004327",
   "metadata": {},
   "source": [
    "# ATTENTION check how it writes a torch tensor and how it is reading it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7e7064e-0bc7-42d4-a482-720a2830e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('word_embeddings.csv', 'w') as f:  # You will need 'wb' mode in Python 2.x\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['key','value'])\n",
    "    for k,v in embeddings_dict.items() :\n",
    "        w.writerow([k,v.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99ebe0f3-30f0-4ad0-8772-e343e6deb63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open('word_embeddings.csv', newline='') as pscfile:\n",
    "    reader = csv.DictReader(pscfile)\n",
    "    for row in reader:\n",
    "        embeddings_dict[row['key']] = torch.from_numpy(numpy.array(row['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5056dd2-573b-407f-868c-d12f3507a231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = get_entities_embeddings('798-8', embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695eac98-52f1-4807-9e0d-f6e30c60468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_type = 'first_version'\n",
    "\n",
    "print('Word2Vec Embeddings - Skip-gram')\n",
    "print_clustering_metrics(embeddings, embeddings_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35df9b1-0480-4524-b6db-de1a8d26e725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
