{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67525bb4-7491-4028-a288-6f8fb3fbf6ee",
   "metadata": {},
   "source": [
    "# Evaluate NER performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40165e0d-85e9-43e7-9450-8bd308ad1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete everything about occupation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d421b-012d-454c-ae1e-7da6cc13e4a7",
   "metadata": {},
   "source": [
    "## TODO :\n",
    "\n",
    "- faire le groundtruth dataset\n",
    "- faire textually close dataset\n",
    "- faire le lax close dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60ff5f78-7dc8-405d-9bc0-249b9970993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../src')\n",
    "\n",
    "# from character_extraction import *\n",
    "from embeddings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c606428e-48d9-4705-b12b-07dcf8ed44f7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['abraham', 'adams', 'affection', 'agen', 'agnes', 'aladdin',\n",
       "       'albion', 'anne', 'annie', 'appetite', 'appy', 'ard', 'art',\n",
       "       'arter', 'aunt', 'babley', 'bailey', 'barkis', 'beadwood', 'bear',\n",
       "       'benjamin', 'betsey', 'blackboy', 'blackstone', 'blas', 'blossom',\n",
       "       'bob', 'bodgers', 'bor', 'boy', 'braid', 'britannia', 'brooks',\n",
       "       'bullock', 'burke', 'burns', 'c', 'caesar', 'cain', 'canning',\n",
       "       'caroline', 'castlereagh', 'cato', 'charity', 'charles', 'charley',\n",
       "       'chaucer', 'chestle', 'child', 'chillip', 'chrisen', 'christ',\n",
       "       'clara', 'clarissa', 'clickett', 'clinker', 'cook', 'copperfield',\n",
       "       'copperfull', 'cosily', 'creakle', 'creetur', 'crewler', 'croesus',\n",
       "       'crow', 'crupp', 'crusoe', 'd', 'daisy', 'dan', 'daniel', 'dartle',\n",
       "       'david', 'davy', 'demple', 'deuce', 'dick', 'dickens', 'dixon',\n",
       "       'doady', 'doctor', 'dolloby', 'don', 'dora', 'dustman', 'edward',\n",
       "       'eight', 'em', 'emily', 'emma', 'endell', 'evermore', 'father',\n",
       "       'fatima', 'fawkes', 'fibbitson', 'fox', 'foxe', 'francis',\n",
       "       'franklin', 'fust', 'gazelle', 'gen', 'george', 'gil', 'gipsy',\n",
       "       'god', 'good', 'goodness', 'goroo', 'gowans', 'grainger',\n",
       "       'grayper', 'gregory', 'grey', 'grinby', 'gulpidge', 'gummidge',\n",
       "       'guy', 'halloa', 'ham', 'hamlet', 'heaven', 'heep', 'helena',\n",
       "       'henry', 'hisself', 'hopkins', 'horace', 'huckster', 'humphrey',\n",
       "       'i', 'inveigler', 'isaac', 'j', 'jack', 'james', 'jane', 'janet',\n",
       "       'jellips', 'jemmy', 'jip', 'joe', 'john', 'johnson', 'jones',\n",
       "       'joram', 'jorkins', 'julia', 'julius', 'justice', 'ketch',\n",
       "       'kidgerbury', 'kiender', 'kind', 'king', 'kitt', 'l', 'larkins',\n",
       "       'late', 'lavinia', 'lawk', 'lazarus', 'linwood', 'littimer',\n",
       "       'little', 'lo', 'lorblessmer', 'lord', 'louisa', 'love', 'lucy',\n",
       "       'ly', 'm', 'macbeth', 'maldon', 'mama', 'man', 'margaret', 'mark',\n",
       "       'markham', 'markleham', 'martha', 'mary', 'mas', 'master',\n",
       "       'masters', 'mawther', 'mealy', 'mell', 'micawber', 'micawbers',\n",
       "       'mick', 'mills', 'minnie', 'miss', 'missing', 'missis', 'mithers',\n",
       "       'moon', 'moppet', 'mortimer', 'mortimers', 'mother', 'mowcher',\n",
       "       'mr', 'murdstone', 'n', 'nat', 'ned', 'nelson', 'nettingall',\n",
       "       'nettingalls', 'noah', 'north', 'nothing', 'old', 'omer',\n",
       "       'oncommon', 'one', 'orfling', 'ostade', 'p', 'painter', 'papa',\n",
       "       'paragon', 'passnidge', 'peggotty', 'peregrine', 'peter',\n",
       "       'phoebus', 'pickle', 'pidger', 'pipes', 'pitt', 'plato', 'polly',\n",
       "       'porter', 'potatoes', 'pray', 'prentice', 'pretty', 'pritty',\n",
       "       'pyegrave', 'quinion', 'quixote', 'r', 'ral', 'random', 'raps',\n",
       "       'red', 'redbreast', 'renorwell', 'richard', 'ridger', 'robin',\n",
       "       'robinson', 'roc', 'roderick', 'rosa', 'rudderford', 's', 'samson',\n",
       "       'sarah', 'satan', 'schoolfellow', 'sermuchser', 'seven',\n",
       "       'shakespeare', 'sharp', 'shepherd', 'sheridan', 'sich', 'sidmouth',\n",
       "       'skylark', 'sol', 'somebody', 'sophy', 'spenlow', 'spenlows',\n",
       "       'spiker', 'spikers', 'steerforth', 'strap', 'strong', 't',\n",
       "       'taters', 'tempest', 'terpsichore', 'thankee', 'theer', 'theerfur',\n",
       "       'thomas', 'tidd', 'tiffey', 'time', 'tipkins', 'tipp', 'tom',\n",
       "       'tommy', 'topsawyer', 'traddles', 'trot', 'trotwood', 'trunnion',\n",
       "       'trusted', 'tungay', 'twenty', 'umble', 'un', 'uncle', 'uriah',\n",
       "       'ury', 'w', 'walker', 'waterbrook', 'watson', 'watts',\n",
       "       'whensoever', 'whisker', 'whomsoever', 'wickfield', 'wilkins',\n",
       "       'william', 'yawler', 'yore', 'young'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = pd.read_csv('../scripts/previous project/notebooks_data/book_dfs/david_copperfield_df.csv')['full_word'].unique()\n",
    "temp.sort()\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e4b75a1-5171-4610-802a-54347e339078",
   "metadata": {},
   "outputs": [],
   "source": [
    "book = get_book_text('798-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6489a79e-e3ea-4e5a-8845-7d23841c07cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "book = re.sub(r\"\"\"\n",
    "               [,.;@#?!&$'-]+  # Accept one or more copies of punctuation\n",
    "               \\ *           # plus zero or more copies of a space,\n",
    "               \"\"\",\n",
    "               \" \",          # and replace it with a single space\n",
    "               book, flags=re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "68e0b368-12ea-489b-92c9-acae728158ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ground_truth_data_df = pd.DataFrame([ \n",
    "('agde', 'Monsieur Agde',13,'M',36930), \n",
    "('altamira','Comte Altamira',26,'M',97835),\n",
    "('amanda', 'Mme Amanda Binet',17,'F',60381),\n",
    "('appert', 'M Appert',8,'M',2363),\n",
    "('castanède','Abbé Castanède',19,'M',64861),\n",
    "('caylus','Comte Caylus',24,'M',94378),\n",
    "('chélan','Abbé Chélan',6,'M',1929),\n",
    "('croisenois','Monsieur Croisenois',23,'M',94374),\n",
    "('danton','Monsieur Danton',1,'M', 15),\n",
    "('derville','Madame Derville',12,'F',13130),\n",
    "('falcoz','Monsieur Falcoz',14,'M',45151),\n",
    "('fervaques','Madame Fervaques',25,'F',96924),\n",
    "('fouqué','Monsieur Fouqué',10,'M',7451),\n",
    "('frilair','Monsieur Frilair',15,'M',53833),\n",
    "('geronimo','Monsieur Geronimo',16,'M',55797),\n",
    "('korasoff','Monsieur Korasoff',27,'M',102772),\n",
    "('julien','Monsieur Julien Sorel',3,'M',4751),\n",
    "('louise','Madame Louise Rênal',7,'F',45391),\n",
    "('maslon','Monsieur Maslon',5,'M',1900),\n",
    "('mathilde','Mademoiselle Mathilde Sorel',21,'F',90709),\n",
    "('norbert','Monsieur Norbert Mole',20,'M',87123),\n",
    "('pirard','Monsieur Pirard',18,'M',62166),\n",
    "('rênal','Monsieur de Rênal',2,'M', 605),\n",
    "('rênal','Madame Louise Rênal',7,'F', 2214),\n",
    "('sorel','Monsieur Julien Sorel',3,'M', 940),\n",
    "('tanbeau','Monsieur Tanbeau',22,'M',92323),\n",
    "('valenod','Monsieur Valenod',4,'M',1724),\n",
    "('élisa','Mademoiselle Élisa',11,'F',12267),\n",
    "('mole', 'Mademoiselle Mathilde Sorel', 21,'F',90768),\n",
    "('mole', 'Monsieur de la Mole',9,'M',2610)],\n",
    "columns=['name', 'entity','entity_ID', 'gender','first_appearance' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7707e3e5-d1c7-4fbb-bcea-86ab3570252d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>entity</th>\n",
       "      <th>entity_ID</th>\n",
       "      <th>gender</th>\n",
       "      <th>first_appearance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>danton</td>\n",
       "      <td>Monsieur Danton</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rênal</td>\n",
       "      <td>Monsieur de Rênal</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sorel</td>\n",
       "      <td>Monsieur Julien Sorel</td>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>valenod</td>\n",
       "      <td>Monsieur Valenod</td>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>1724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>maslon</td>\n",
       "      <td>Monsieur Maslon</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chélan</td>\n",
       "      <td>Abbé Chélan</td>\n",
       "      <td>6</td>\n",
       "      <td>M</td>\n",
       "      <td>1929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rênal</td>\n",
       "      <td>Madame Louise Rênal</td>\n",
       "      <td>7</td>\n",
       "      <td>F</td>\n",
       "      <td>2214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>appert</td>\n",
       "      <td>M Appert</td>\n",
       "      <td>8</td>\n",
       "      <td>M</td>\n",
       "      <td>2363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>mole</td>\n",
       "      <td>Monsieur de la Mole</td>\n",
       "      <td>9</td>\n",
       "      <td>M</td>\n",
       "      <td>2610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>julien</td>\n",
       "      <td>Monsieur Julien Sorel</td>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>4751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fouqué</td>\n",
       "      <td>Monsieur Fouqué</td>\n",
       "      <td>10</td>\n",
       "      <td>M</td>\n",
       "      <td>7451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>élisa</td>\n",
       "      <td>Mademoiselle Élisa</td>\n",
       "      <td>11</td>\n",
       "      <td>F</td>\n",
       "      <td>12267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>derville</td>\n",
       "      <td>Madame Derville</td>\n",
       "      <td>12</td>\n",
       "      <td>F</td>\n",
       "      <td>13130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>agde</td>\n",
       "      <td>Monsieur Agde</td>\n",
       "      <td>13</td>\n",
       "      <td>M</td>\n",
       "      <td>36930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>falcoz</td>\n",
       "      <td>Monsieur Falcoz</td>\n",
       "      <td>14</td>\n",
       "      <td>M</td>\n",
       "      <td>45151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>louise</td>\n",
       "      <td>Madame Louise Rênal</td>\n",
       "      <td>7</td>\n",
       "      <td>F</td>\n",
       "      <td>45391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>frilair</td>\n",
       "      <td>Monsieur Frilair</td>\n",
       "      <td>15</td>\n",
       "      <td>M</td>\n",
       "      <td>53833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>geronimo</td>\n",
       "      <td>Monsieur Geronimo</td>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>55797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amanda</td>\n",
       "      <td>Mme Amanda Binet</td>\n",
       "      <td>17</td>\n",
       "      <td>F</td>\n",
       "      <td>60381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pirard</td>\n",
       "      <td>Monsieur Pirard</td>\n",
       "      <td>18</td>\n",
       "      <td>M</td>\n",
       "      <td>62166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>castanède</td>\n",
       "      <td>Abbé Castanède</td>\n",
       "      <td>19</td>\n",
       "      <td>M</td>\n",
       "      <td>64861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>norbert</td>\n",
       "      <td>Monsieur Norbert Mole</td>\n",
       "      <td>20</td>\n",
       "      <td>M</td>\n",
       "      <td>87123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>mathilde</td>\n",
       "      <td>Mademoiselle Mathilde Sorel</td>\n",
       "      <td>21</td>\n",
       "      <td>F</td>\n",
       "      <td>90709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mole</td>\n",
       "      <td>Mademoiselle Mathilde Sorel</td>\n",
       "      <td>21</td>\n",
       "      <td>F</td>\n",
       "      <td>90768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tanbeau</td>\n",
       "      <td>Monsieur Tanbeau</td>\n",
       "      <td>22</td>\n",
       "      <td>M</td>\n",
       "      <td>92323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>croisenois</td>\n",
       "      <td>Monsieur Croisenois</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>94374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>caylus</td>\n",
       "      <td>Comte Caylus</td>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>94378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>fervaques</td>\n",
       "      <td>Madame Fervaques</td>\n",
       "      <td>25</td>\n",
       "      <td>F</td>\n",
       "      <td>96924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>altamira</td>\n",
       "      <td>Comte Altamira</td>\n",
       "      <td>26</td>\n",
       "      <td>M</td>\n",
       "      <td>97835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>korasoff</td>\n",
       "      <td>Monsieur Korasoff</td>\n",
       "      <td>27</td>\n",
       "      <td>M</td>\n",
       "      <td>102772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name                       entity  entity_ID gender  \\\n",
       "8       danton              Monsieur Danton          1      M   \n",
       "22       rênal            Monsieur de Rênal          2      M   \n",
       "24       sorel        Monsieur Julien Sorel          3      M   \n",
       "26     valenod             Monsieur Valenod          4      M   \n",
       "18      maslon              Monsieur Maslon          5      M   \n",
       "6       chélan                  Abbé Chélan          6      M   \n",
       "23       rênal          Madame Louise Rênal          7      F   \n",
       "3       appert                     M Appert          8      M   \n",
       "29        mole          Monsieur de la Mole          9      M   \n",
       "16      julien        Monsieur Julien Sorel          3      M   \n",
       "12      fouqué              Monsieur Fouqué         10      M   \n",
       "27       élisa           Mademoiselle Élisa         11      F   \n",
       "9     derville              Madame Derville         12      F   \n",
       "0         agde                Monsieur Agde         13      M   \n",
       "10      falcoz              Monsieur Falcoz         14      M   \n",
       "17      louise          Madame Louise Rênal          7      F   \n",
       "13     frilair             Monsieur Frilair         15      M   \n",
       "14    geronimo            Monsieur Geronimo         16      M   \n",
       "2       amanda             Mme Amanda Binet         17      F   \n",
       "21      pirard              Monsieur Pirard         18      M   \n",
       "4    castanède               Abbé Castanède         19      M   \n",
       "20     norbert        Monsieur Norbert Mole         20      M   \n",
       "19    mathilde  Mademoiselle Mathilde Sorel         21      F   \n",
       "28        mole  Mademoiselle Mathilde Sorel         21      F   \n",
       "25     tanbeau             Monsieur Tanbeau         22      M   \n",
       "7   croisenois          Monsieur Croisenois         23      M   \n",
       "5       caylus                 Comte Caylus         24      M   \n",
       "11   fervaques             Madame Fervaques         25      F   \n",
       "1     altamira               Comte Altamira         26      M   \n",
       "15    korasoff            Monsieur Korasoff         27      M   \n",
       "\n",
       "    first_appearance  \n",
       "8                 15  \n",
       "22               605  \n",
       "24               940  \n",
       "26              1724  \n",
       "18              1900  \n",
       "6               1929  \n",
       "23              2214  \n",
       "3               2363  \n",
       "29              2610  \n",
       "16              4751  \n",
       "12              7451  \n",
       "27             12267  \n",
       "9              13130  \n",
       "0              36930  \n",
       "10             45151  \n",
       "17             45391  \n",
       "13             53833  \n",
       "14             55797  \n",
       "2              60381  \n",
       "21             62166  \n",
       "4              64861  \n",
       "20             87123  \n",
       "19             90709  \n",
       "28             90768  \n",
       "25             92323  \n",
       "7              94374  \n",
       "5              94378  \n",
       "11             96924  \n",
       "1              97835  \n",
       "15            102772  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_data_df.sort_values(by='first_appearance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "967ed151-f492-4a91-8da5-547462321b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clustering_metrics(embeddings, embeddings_type):\n",
    "    '''Given embeddings, and their ground truth data type, computes several clustering performance\n",
    "    metrics. The right `ground_truth_data_df`, `textually_close_ent_ground_truth_df` or \n",
    "    `lax_ent_ground_truth_df` should have been loaded into memory before calling this function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings : dictionary\n",
    "        The dictionary containing each entity and their associated embedding vector\n",
    "    embeddings_type : str\n",
    "        The matching ground truth data type for the given embeddings (either 'first_version',\n",
    "        'textually_close' or 'lax')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    same_entityness : list\n",
    "        A list containing the performance metrics with regards to the 'same_entityness' axis\n",
    "    gender : list\n",
    "        A list containing the performance metrics with regards to the 'gender' axis\n",
    "    first_appearance : list\n",
    "        A list containing the performance metrics with regards to the 'first_appearance' axis\n",
    "    '''\n",
    "    \n",
    "    # SAME ENTITY-NESS\n",
    "    same_entityness = []\n",
    "    \n",
    "    if embeddings_type == 'first_version':\n",
    "        mask_embs_entity = [(k, \n",
    "                             embeddings[k], \n",
    "                             ground_truth_data_df[ground_truth_data_df['name'] == k]['entity_ID'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k.lower() in ground_truth_data_df['name'].tolist()]\n",
    "    elif embeddings_type == 'textually_close':\n",
    "        mask_embs_entity = [(k, \n",
    "                             embeddings[k]['MASK'], \n",
    "                             textually_close_ent_ground_truth_df[textually_close_ent_ground_truth_df['name'] == k]['entity_ID'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k in textually_close_ent_ground_truth_df['name'].tolist()]\n",
    "    elif embeddings_type == 'lax':\n",
    "        mask_embs_entity = [(k, \n",
    "                             embeddings[k]['MASK'], \n",
    "                             lax_ent_ground_truth_df[lax_ent_ground_truth_df['name'] == k]['entity_ID'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k in lax_ent_ground_truth_df['name'].tolist()]\n",
    "        \n",
    "    tmp_df = pd.DataFrame(mask_embs_entity)\n",
    "    same_entityness.append(sklearn.metrics.silhouette_score(np.array(tmp_df[1].tolist()), \n",
    "                                                            np.array(tmp_df[2]), \n",
    "                                                            metric='euclidean', \n",
    "                                                            random_state=0))\n",
    "    \n",
    "    same_entityness.append(sklearn.metrics.calinski_harabasz_score(np.array(tmp_df[1].tolist()), \n",
    "                                                                   np.array(tmp_df[2])))\n",
    "    \n",
    "    same_entityness.append(sklearn.metrics.davies_bouldin_score(np.array(tmp_df[1].tolist()), \n",
    "                                                                np.array(tmp_df[2])))\n",
    "    \n",
    "    tmp_df = pd.DataFrame(mask_embs_entity)\n",
    "    entityness_matrix = np.array([np.array(emb) for emb in tmp_df[1]])\n",
    "    k_choice = 45 # obtained by the elbow method\n",
    "    kmean = KMeans(n_clusters=k_choice, random_state=0).fit(entityness_matrix, )\n",
    "    predicted_clusters = kmean.predict(np.array([np.array(emb) for emb in tmp_df[1]]))\n",
    "    \n",
    "    same_entityness.append(sklearn.metrics.rand_score(np.array(tmp_df[2]), predicted_clusters))\n",
    "    same_entityness.append(sklearn.metrics.adjusted_rand_score(np.array(tmp_df[2]), predicted_clusters))\n",
    "    same_entityness.append(sklearn.metrics.mutual_info_score(np.array(tmp_df[2]), predicted_clusters))\n",
    "    same_entityness.append(sklearn.metrics.adjusted_mutual_info_score(np.array(tmp_df[2]), \n",
    "                                                                      predicted_clusters, \n",
    "                                                                      average_method='arithmetic'))\n",
    "    \n",
    "    \n",
    "    # GENDER\n",
    "    gender = []\n",
    "    \n",
    "    if embeddings_type == 'first_version':\n",
    "        mask_embs_gender = [(k, \n",
    "                             embeddings[k], \n",
    "                             ground_truth_data_df[ground_truth_data_df['name'] == k]['gender'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k.lower() in ground_truth_data_df['name'].tolist()]\n",
    "    elif embeddings_type == 'textually_close':\n",
    "        mask_embs_gender = [(k, \n",
    "                             embeddings[k]['MASK'], \n",
    "                             textually_close_ent_ground_truth_df[textually_close_ent_ground_truth_df['name'] == k]['gender'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k in textually_close_ent_ground_truth_df['name'].tolist()]\n",
    "    elif embeddings_type == 'lax':\n",
    "        mask_embs_gender = [(k, \n",
    "                             embeddings[k]['MASK'], \n",
    "                             lax_ent_ground_truth_df[lax_ent_ground_truth_df['name'] == k]['gender'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k in lax_ent_ground_truth_df['name'].tolist()]\n",
    "\n",
    "    tmp_df = pd.DataFrame(mask_embs_gender)\n",
    "    gender.append(sklearn.metrics.silhouette_score(np.array(tmp_df[1].tolist()), \n",
    "                                                   np.array(tmp_df[2] == 'M').astype(int), \n",
    "                                                   metric='euclidean', \n",
    "                                                   random_state=0))\n",
    "    gender.append(sklearn.metrics.calinski_harabasz_score(np.array(tmp_df[1].tolist()), np.array(tmp_df[2])))\n",
    "    gender.append(sklearn.metrics.davies_bouldin_score(np.array(tmp_df[1].tolist()), np.array(tmp_df[2])))\n",
    "    \n",
    "    tmp_df = pd.DataFrame(mask_embs_gender)\n",
    "    gender_matrix = np.array([np.array(emb) for emb in tmp_df[1]])\n",
    "    k_choice = 2 # two genders in PG literature (men and women)\n",
    "    kmean = KMeans(n_clusters=k_choice, random_state=0).fit(gender_matrix)\n",
    "    predicted_clusters = kmean.predict(np.array([np.array(emb) for emb in tmp_df[1]]))\n",
    "    \n",
    "    gender.append(sklearn.metrics.rand_score(np.array(tmp_df[2]), predicted_clusters))\n",
    "    gender.append(sklearn.metrics.adjusted_rand_score(np.array(tmp_df[2]), predicted_clusters))\n",
    "    gender.append(sklearn.metrics.mutual_info_score(np.array(tmp_df[2]), predicted_clusters))\n",
    "    gender.append(sklearn.metrics.adjusted_mutual_info_score(np.array(tmp_df[2]), predicted_clusters, \n",
    "                                                             average_method='arithmetic'))\n",
    "    \n",
    "    # FIRST APPEARANCE\n",
    "    first_appearance = []\n",
    "    \n",
    "    # build distance matrix \n",
    "    if embeddings_type == 'first_version':\n",
    "        mask_embs_appear = [(k, \n",
    "                             embeddings[k], \n",
    "                             ground_truth_data_df[ground_truth_data_df['name'] == k]['first appearance'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k.lower() in ground_truth_data_df['name'].tolist()]\n",
    "    elif embeddings_type == 'textually_close':\n",
    "        mask_embs_appear = [(k, \n",
    "                             embeddings[k]['MASK'], \n",
    "                             textually_close_ent_ground_truth_df[textually_close_ent_ground_truth_df['name'] == k]['first appearance'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k in textually_close_ent_ground_truth_df['name'].tolist()]\n",
    "    elif embeddings_type == 'lax':\n",
    "        mask_embs_appear = [(k, \n",
    "                             embeddings[k]['MASK'], \n",
    "                             lax_ent_ground_truth_df[lax_ent_ground_truth_df['name'] == k]['first appearance'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k in lax_ent_ground_truth_df['name'].tolist()]\n",
    "        \n",
    "    tmp_df = pd.DataFrame(mask_embs_appear)\n",
    "    appear_matrix = np.array(tmp_df[2]).reshape(-1, 1)\n",
    "\n",
    "    # k based both on \"vector\" being predict (first appearance in book) and overall clustering\n",
    "    # using elbow shape\n",
    "    k_choice = 17\n",
    "    kmean = KMeans(n_clusters=k_choice, random_state=0).fit(appear_matrix)\n",
    "\n",
    "    first_appearance.append(sklearn.metrics.silhouette_score(np.array(tmp_df[1].tolist()), \n",
    "                                         kmean.predict(np.array(tmp_df[2]).reshape(-1,1)), \n",
    "                                         metric='euclidean', \n",
    "                                         random_state=0))\n",
    "    \n",
    "    first_appearance.append(sklearn.metrics.calinski_harabasz_score(np.array(tmp_df[1].tolist()), \n",
    "                                 kmean.predict(np.array(tmp_df[2]).reshape(-1,1))))\n",
    "    \n",
    "    first_appearance.append(sklearn.metrics.davies_bouldin_score(np.array(tmp_df[1].tolist()), \n",
    "                                 kmean.predict(np.array(tmp_df[2]).reshape(-1,1))))\n",
    "    \n",
    "    tmp_df = pd.DataFrame(mask_embs_appear)\n",
    "    ground_truth_based_clusters = kmean.predict(np.array(tmp_df[2]).reshape(-1,1))\n",
    "    appear_matrix = np.array([np.array(emb) for emb in tmp_df[1]])\n",
    "    kmean = KMeans(n_clusters=k_choice, random_state=0).fit(appear_matrix)\n",
    "    predicted_clusters = kmean.predict(np.array([np.array(emb) for emb in tmp_df[1]]))\n",
    "    \n",
    "    first_appearance.append(sklearn.metrics.rand_score(ground_truth_based_clusters, predicted_clusters))\n",
    "    first_appearance.append(sklearn.metrics.adjusted_rand_score(ground_truth_based_clusters, predicted_clusters))\n",
    "    first_appearance.append(sklearn.metrics.mutual_info_score(ground_truth_based_clusters, predicted_clusters))\n",
    "    first_appearance.append(sklearn.metrics.adjusted_mutual_info_score(ground_truth_based_clusters, predicted_clusters, \n",
    "                                                                       average_method='arithmetic'))\n",
    "    \n",
    "    return same_entityness, gender, first_appearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b86dbd94-1ab5-4bbe-91b4-41dcb7f8009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clustering_metrics(embeddings, embeddings_type):\n",
    "    '''Given embeddings, and their ground truth data type, display in a table several\n",
    "    clustering performance metrics. The right `ground_truth_data_df`, \n",
    "    `textually_close_ent_ground_truth_df` or `lax_ent_ground_truth_df` should have been \n",
    "    loaded into memory before calling this function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings : dictionary\n",
    "        The dictionary containing each entity and their associated embedding vector\n",
    "    embeddings_type : str\n",
    "        The matching ground truth data type for the given embeddings (either 'first_version',\n",
    "        'textually_close' or 'lax')\n",
    "    '''\n",
    "    \n",
    "    same_entityness, gender, first_appearance = get_clustering_metrics(embeddings, embeddings_type)\n",
    "    print('--------------------------------------------------------------------------------------------')\n",
    "    print('|                            | Same Entity-ness |  Gender  | First Appearance |')\n",
    "    print('--------------------------------------------------------------------------------------------')\n",
    "    print(f'| Silhouette Score           |     {same_entityness[0]:8.5f}     | {gender[0]:8.5f} |  {first_appearance[0]:8.5f}     |')\n",
    "    print(f'| Calinski Harabasz Score    |     {same_entityness[1]:8.5f}     | {gender[1]:8.5f} |  {first_appearance[1]:8.5f}     |')\n",
    "    print(f'| Davies Bouldin Score       |     {same_entityness[2]:8.5f}     | {gender[2]:8.5f} |  {first_appearance[2]:8.5f}     |')\n",
    "    print(f'| Rand Score                 |     {same_entityness[3]:8.5f}     | {gender[3]:8.5f} |  {first_appearance[3]:8.5f}     |')\n",
    "    print(f'| Adjusted Rand Score        |     {same_entityness[4]:8.5f}     | {gender[4]:8.5f} |  {first_appearance[4]:8.5f}     |')\n",
    "    print(f'| Mutual Info Score          |     {same_entityness[5]:8.5f}     | {gender[5]:8.5f} |  {first_appearance[5]:8.5f}     |')\n",
    "    print(f'| Adjusted Mutual Info Score |     {same_entityness[6]:8.5f}     | {gender[6]:8.5f} |  {first_appearance[6]:8.5f}     |')\n",
    "    print('--------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca813ee6-8b9e-48ae-8d27-5f4dc4a4699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_entityness = []\n",
    "    \n",
    "if embeddings_type == 'first_version':\n",
    "    mask_embs_entity = [(k, \n",
    "                         embeddings[k], \n",
    "                         ground_truth_data_df[ground_truth_data_df['name'] == k]['entity_ID'].values[0]) \n",
    "                        for k in embeddings \n",
    "                        if k.lower() in ground_truth_data_df['name'].tolist()]\n",
    "elif embeddings_type == 'textually_close':\n",
    "    mask_embs_entity = [(k, \n",
    "                         embeddings[k]['MASK'], \n",
    "                         textually_close_ent_ground_truth_df[textually_close_ent_ground_truth_df['name'] == k]['entity_ID'].values[0]) \n",
    "                        for k in embeddings \n",
    "                        if k in textually_close_ent_ground_truth_df['name'].tolist()]\n",
    "elif embeddings_type == 'lax':\n",
    "    mask_embs_entity = [(k, \n",
    "                         embeddings[k]['MASK'], \n",
    "                         lax_ent_ground_truth_df[lax_ent_ground_truth_df['name'] == k]['entity_ID'].values[0]) \n",
    "                        for k in embeddings \n",
    "                        if k in lax_ent_ground_truth_df['name'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa4e9982-3568-4f2b-a461-568d6f935ace",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at flaubert/flaubert_base_cased were not used when initializing FlaubertModel: ['pred_layer.proj.weight', 'pred_layer.proj.bias']\n",
      "- This IS expected if you are initializing FlaubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|▏                                                                                                                    | 13/7929 [00:32<5:28:00,  2.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3962115/1994861408.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeddings_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrench_word_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'flaubert/flaubert_base_cased'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'798-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/students/doyard/sentiment-evolution-in-characters-network/scripts/../src/embeddings.py\u001b[0m in \u001b[0;36mfrench_word_embeddings\u001b[0;34m(model_name, gutenberg_id)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprocessed_sentenced\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflaubert_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_sentenced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflaubert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_sentenced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/students/doyard/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/students/doyard/lib/python3.8/site-packages/transformers/models/flaubert/modeling_flaubert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, langs, token_type_ids, position_ids, lengths, cache, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;31m# self attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_norm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 attn_outputs = self.attentions[i](\n\u001b[0m\u001b[1;32m    264\u001b[0m                     \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                     \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/students/doyard/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/students/doyard/lib/python3.8/site-packages/transformers/models/xlm/modeling_xlm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, mask, kv, cache, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdim_per_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_lin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, qlen, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_lin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, qlen, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/students/doyard/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/students/doyard/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddings_dict = french_word_embeddings('flaubert/flaubert_base_cased', '798-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f2f09-ceee-4adc-9a68-f428b6004327",
   "metadata": {},
   "source": [
    "# ATTENTION check how it writes a torch tensor and how it is reading it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7e7064e-0bc7-42d4-a482-720a2830e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('word_embeddings.csv', 'w') as f:  # You will need 'wb' mode in Python 2.x\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['key','value'])\n",
    "    for k,v in embeddings_dict.items() :\n",
    "        w.writerow([k,v.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99ebe0f3-30f0-4ad0-8772-e343e6deb63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open('word_embeddings.csv', newline='') as pscfile:\n",
    "    reader = csv.DictReader(pscfile)\n",
    "    for row in reader:\n",
    "        embeddings_dict[row['key']] = torch.from_numpy(numpy.array(row['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5056dd2-573b-407f-868c-d12f3507a231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = get_entities_embeddings('798-8', embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695eac98-52f1-4807-9e0d-f6e30c60468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_type = 'first_version'\n",
    "\n",
    "print('Word2Vec Embeddings - Skip-gram')\n",
    "print_clustering_metrics(embeddings, embeddings_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35df9b1-0480-4524-b6db-de1a8d26e725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
