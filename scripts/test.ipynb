{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2057c890-73f3-4c01-9762-5dba08d2221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/eloisedoyard/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/eloisedoyard/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/eloisedoyard/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/eloisedoyard/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/eloisedoyard/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from transformers import BertConfig, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, \\\n",
    "                         AutoTokenizer, AutoModelForTokenClassification, pipeline, BertTokenizer, BertModel, \\\n",
    "                         LukeTokenizer, LukeForEntitySpanClassification\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "328b7698-9fdb-4559-8663-c06c4e41d473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.tokenization_bert.BertTokenizer"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import unicodedata\n",
    "from functools import reduce\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from nameparser import HumanName\n",
    "from nameparser.config import Constants\n",
    "\n",
    "import fuzzy\n",
    "import string\n",
    "import editdistance\n",
    "from libindic.soundex import Soundex\n",
    "from stop_words import get_stop_words\n",
    "#from python_parser import NameDenormalizer\n",
    "from pyjarowinkler.distance import get_jaro_distance\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import BertConfig, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, \\\n",
    "                         AutoTokenizer, AutoModelForTokenClassification, pipeline, BertTokenizer, BertModel, \\\n",
    "                         LukeTokenizer, LukeForEntitySpanClassification\n",
    "BertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47cd8d05-d155-4028-b537-9a7016eda5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertModel, CamembertTokenizer\n",
    "from transformers import FlaubertTokenizer, FlaubertModel, FlaubertForTokenClassification\n",
    "from transformers import CamembertTokenizer, CamembertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ba66e2-6043-463f-acb8-3f59d30ce3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_text(gutenberg_id):\n",
    "    '''Given a book ID, returns the book's text, excluding Project Gutenberg's header and outro.\n",
    "    \n",
    "     Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    book_text : str\n",
    "        The book's text, excluding Project Gutenberg's header and outro\n",
    "    '''\n",
    "    \n",
    "    context = ''\n",
    "    with open(f'../data/book/PG-{gutenberg_id}.txt', mode='r', encoding='utf-8') as f:\n",
    "        context = f.read()\n",
    "    return ' '.join([l for l in (context.split('End of the Project Gutenberg EBook of ')[0]\n",
    "                                        .split('*** END OF THE PROJECT GUTENBERG EBOOK')[0]\n",
    "                                        .split('\\n')) if l][16:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b16be97-d9bd-4501-a14a-b0833c337e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_stopwords = pd.read_csv('../data/stopwords-fr.txt', header = None)[0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6477ccb4-09a1-46e9-b628-e6d4f4434155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_entities(l, ner_entities_tokens, ner_entities_words, sentence_index, tokenizer, nlp,\n",
    "                     grouped_entities):\n",
    "    '''Given a line, lists for tokens and words, and word index at the end of the sentence, as well as\n",
    "    the tokenizer and nlp model instances (from huggingface's transformers), updates the tokens and\n",
    "    words lists and the word index to include the given line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    l : str\n",
    "        The line to analyze\n",
    "    ner_entities_tokens : list\n",
    "        A list containing all the Person tokens found so far, across all the previous lines\n",
    "    ner_entities_words : list\n",
    "        A list containing dictionary entries of all the Person entities found so far (the full \n",
    "        word corresponding to them (i.e. not separated tokens), their index in the sentence and \n",
    "        in the book overall, and their PER-entity classification score, a number between 0.0 and \n",
    "        1.0), across all the previous lines\n",
    "    sentence_index : int\n",
    "        The overall (book-wise) index of the first word of the sentence\n",
    "    tokenizer : AutoTokenizer\n",
    "        huggingface's tokenizer being used in the NER pipeline\n",
    "    nlp : pipeline\n",
    "        huggingface's NER pipeline object\n",
    "    grouped_entities : bool\n",
    "        Flag indicating whether the NER pipeline is configured to output grouped_entities or not\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ner_entities_tokens : list\n",
    "        A list containing all the Person tokens found so far, across this and all the previous lines\n",
    "    ner_entities_words : list\n",
    "        A list containing dictionary entries of all the Person entities found so far (the full \n",
    "        word corresponding to them (i.e. not separated tokens), their index in the sentence and \n",
    "        in the book overall, and their PER-entity classification score, a number between 0.0 and \n",
    "        1.0), across this and all the previous lines\n",
    "    sentence_index : int\n",
    "        The overall (book-wise) index of the first word of the next sentence\n",
    "    '''\n",
    "    new_entity_tokens = []\n",
    "    if grouped_entities:\n",
    "        new_entity_tokens = [e for e in nlp(l) if 'PER' in e['entity_group']]\n",
    "    else:\n",
    "        new_entity_tokens = [e for e in nlp(l) if 'PER' in e['entity']]\n",
    "    ner_entities_tokens += new_entity_tokens\n",
    "    # print('new entity tokens')\n",
    "    # print(new_entity_tokens)\n",
    "    tokenized_line = tokenizer(l)\n",
    "    line_words = [w if w != 'word_tokenize_splits_cannot_into_2_words' else 'cannot'\n",
    "                    for w in word_tokenize(\n",
    "                                           re.sub(r'[^a-zA-Z0-9À-ÿ]', ' \\g<0> ', \n",
    "                                                  l).replace('cannot', \n",
    "                                                            'word_tokenize_splits_cannot_into_2_words'))]\n",
    "    # print('line words')\n",
    "    # print(line_words)\n",
    "    # go from token to word with\n",
    "    for et in new_entity_tokens:\n",
    "        if grouped_entities:\n",
    "            # find index of grouped entity\n",
    "            reconstructed_line = ' '.join([lw.lower() for lw in line_words])\n",
    "            first_word = word_tokenize(re.sub(r'[^a-zA-Z0-9_À-ÿ]', ' \\g<0> ', et['word']))[0]\n",
    "            if et['word'][0] == '#':\n",
    "                first_word = word_tokenize(re.sub(r'[^a-zA-Z0-9_À-ÿ]', ' \\g<0> ', et['word'][2:]))[0]\n",
    "            \n",
    "            word_index = len(reconstructed_line[:reconstructed_line.index(first_word)].split())\n",
    "\n",
    "            if et['word'] not in french_stopwords and et['word'].isalpha():\n",
    "                # record grouped entity\n",
    "                ner_entities_words += [{'full_word': et['word'], \n",
    "                                        'sentence_word_index': word_index, \n",
    "                                        'total_word_index': sentence_index+word_index,\n",
    "                                        'score': et['score']}]\n",
    "        else:\n",
    "            # record non-grouped entity\n",
    "            word_index = tokenized_line.word_ids()[et['index']]\n",
    "            if line_words[word_index] not in french_stopwords and line_words[word_index].isalpha():\n",
    "                ner_entities_words += [{'full_word': line_words[word_index], \n",
    "                                        'sentence_word_index': word_index, \n",
    "                                        'total_word_index': sentence_index+word_index,\n",
    "                                        'score': et['score']}]\n",
    "    sentence_index += len(line_words)\n",
    "    return ner_entities_tokens, ner_entities_words, sentence_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6abdc6b2-0a30-47ea-9f26-695f227086e1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-LOC',\n",
       "  'score': 0.55669767,\n",
       "  'index': 10,\n",
       "  'word': '▁d',\n",
       "  'start': 36,\n",
       "  'end': 38},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.95578015,\n",
       "  'index': 12,\n",
       "  'word': 'Italie',\n",
       "  'start': 39,\n",
       "  'end': 45},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9981791,\n",
       "  'index': 16,\n",
       "  'word': '▁V',\n",
       "  'start': 55,\n",
       "  'end': 57},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9979278,\n",
       "  'index': 17,\n",
       "  'word': 'errière',\n",
       "  'start': 57,\n",
       "  'end': 64},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99829894,\n",
       "  'index': 18,\n",
       "  'word': 's',\n",
       "  'start': 64,\n",
       "  'end': 65},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9855219,\n",
       "  'index': 31,\n",
       "  'word': '▁M',\n",
       "  'start': 112,\n",
       "  'end': 114},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9845025,\n",
       "  'index': 32,\n",
       "  'word': '.',\n",
       "  'start': 114,\n",
       "  'end': 115},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.8523049,\n",
       "  'index': 33,\n",
       "  'word': '▁le',\n",
       "  'start': 115,\n",
       "  'end': 118},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.87298524,\n",
       "  'index': 34,\n",
       "  'word': '▁maire',\n",
       "  'start': 118,\n",
       "  'end': 124},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.92345726,\n",
       "  'index': 74,\n",
       "  'word': '▁M',\n",
       "  'start': 261,\n",
       "  'end': 263},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.91418386,\n",
       "  'index': 75,\n",
       "  'word': '.',\n",
       "  'start': 263,\n",
       "  'end': 264},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9488357,\n",
       "  'index': 76,\n",
       "  'word': '▁de',\n",
       "  'start': 264,\n",
       "  'end': 267},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99203897,\n",
       "  'index': 77,\n",
       "  'word': '▁R',\n",
       "  'start': 267,\n",
       "  'end': 269},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99149936,\n",
       "  'index': 78,\n",
       "  'word': 'ê',\n",
       "  'start': 269,\n",
       "  'end': 270},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9911446,\n",
       "  'index': 79,\n",
       "  'word': 'nal',\n",
       "  'start': 270,\n",
       "  'end': 273},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9943797,\n",
       "  'index': 96,\n",
       "  'word': '▁Légion',\n",
       "  'start': 355,\n",
       "  'end': 362},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9943036,\n",
       "  'index': 97,\n",
       "  'word': '▁d',\n",
       "  'start': 362,\n",
       "  'end': 364},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9942684,\n",
       "  'index': 99,\n",
       "  'word': 'honneur',\n",
       "  'start': 365,\n",
       "  'end': 372}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9440d04f-a0df-412f-9d56-00b2628bf528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "book_text =  get_book_text('798-8')\n",
    "ner_model = 'mrm8488/mobilebert-finetuned-ner'\n",
    "tokenizer = AutoTokenizer.from_pretrained(ner_model)\n",
    "model = AutoModelForTokenClassification.from_pretrained(ner_model)\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "sentence_level_book = re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', book_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "155876e1-9216-4731-bfd5-a0db0b79e9be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new entity tokens\n",
      "[{'entity': 'I-PER', 'score': 0.9980604, 'index': 42, 'word': 'm', 'start': 113, 'end': 114}, {'entity': 'I-PER', 'score': 0.9974752, 'index': 44, 'word': 'le', 'start': 116, 'end': 118}, {'entity': 'I-PER', 'score': 0.93767214, 'index': 45, 'word': 'mai', 'start': 119, 'end': 122}, {'entity': 'I-PER', 'score': 0.906227, 'index': 46, 'word': '##re', 'start': 122, 'end': 124}, {'entity': 'I-PER', 'score': 0.9871251, 'index': 48, 'word': 'jacob', 'start': 126, 'end': 131}, {'entity': 'I-PER', 'score': 0.990449, 'index': 49, 'word': '##in', 'start': 131, 'end': 133}, {'entity': 'I-PER', 'score': 0.955765, 'index': 51, 'word': 'bon', 'start': 137, 'end': 140}, {'entity': 'I-PER', 'score': 0.8932861, 'index': 54, 'word': '##ste', 'start': 146, 'end': 149}, {'entity': 'I-PER', 'score': 0.9953166, 'index': 97, 'word': 'm', 'start': 262, 'end': 263}, {'entity': 'I-PER', 'score': 0.9966055, 'index': 99, 'word': 'de', 'start': 265, 'end': 267}, {'entity': 'I-PER', 'score': 0.9961195, 'index': 100, 'word': 'renal', 'start': 268, 'end': 273}]\n",
      "line words\n",
      "['Un', 'vieux', 'chirurgien', '-', 'major', 'de', 'l', \"'\", 'armée', 'd', \"'\", 'Italie', ',', 'retiré', 'à', 'Verrières', ',', 'et', 'qui', 'de', 'son', 'vivant', 'était', 'à', 'la', 'fois', ',', 'suivant', 'M', '.', 'le', 'maire', ',', 'jacobin', 'et', 'bonapartiste', ',', 'osa', 'bien', 'un', 'jour', 'se', 'plaindre', 'à', 'lui', 'de', 'la', 'mutilation', 'périodique', 'de', 'ces', 'beaux', 'arbres', '.', '-', '-', 'J', \"'\", 'aime', 'l', \"'\", 'ombre', ',', 'répondit', 'M', '.', 'de', 'Rênal', 'avec', 'la', 'nuance', 'de', 'hauteur', 'convenable', 'quand', 'on', 'parle', 'à', 'un', 'chirurgien', ',', 'membre', 'de', 'la', 'Légion', 'd', \"'\", 'honneur', ',', 'j', \"'\", 'aime', 'l', \"'\", 'ombre', ',', 'je', 'fais', 'tailler', 'mes', 'arbres', 'pour', 'donner', 'de', 'l', \"'\", 'ombre', ',', 'et', 'je', 'ne', 'conçois', 'pas', 'qu', \"'\", 'un', 'arbre', 'soit', 'fait', 'pour', 'autre', 'chose', ',', 'quand', 'toutefois', ',', 'comme', 'l', \"'\", 'utile', 'noyer', ',', 'il', '_', 'ne', 'rapporte', 'pas', 'de', 'revenu', '_', '.']\n"
     ]
    }
   ],
   "source": [
    "l=sentence_level_book[65]\n",
    "a,b,c = get_line_entities(l, [], [], 0, tokenizer, nlp,\n",
    "                     False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f606ee3d-4bba-457d-ba8d-85c41ae0526b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': '▁d',\n",
       "  'score': 0.5566989183425903,\n",
       "  'entity': 'I-LOC',\n",
       "  'index': 10,\n",
       "  'start': 36,\n",
       "  'end': 38},\n",
       " {'word': 'Italie',\n",
       "  'score': 0.9557802677154541,\n",
       "  'entity': 'I-LOC',\n",
       "  'index': 12,\n",
       "  'start': 39,\n",
       "  'end': 45},\n",
       " {'word': '▁V',\n",
       "  'score': 0.998179018497467,\n",
       "  'entity': 'I-LOC',\n",
       "  'index': 16,\n",
       "  'start': 55,\n",
       "  'end': 57},\n",
       " {'word': 'errière',\n",
       "  'score': 0.9979278445243835,\n",
       "  'entity': 'I-LOC',\n",
       "  'index': 17,\n",
       "  'start': 57,\n",
       "  'end': 64},\n",
       " {'word': 's',\n",
       "  'score': 0.9982990026473999,\n",
       "  'entity': 'I-LOC',\n",
       "  'index': 18,\n",
       "  'start': 64,\n",
       "  'end': 65},\n",
       " {'word': '▁M',\n",
       "  'score': 0.9855217337608337,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 31,\n",
       "  'start': 112,\n",
       "  'end': 114},\n",
       " {'word': '.',\n",
       "  'score': 0.9845024347305298,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 32,\n",
       "  'start': 114,\n",
       "  'end': 115},\n",
       " {'word': '▁le',\n",
       "  'score': 0.8523029685020447,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 33,\n",
       "  'start': 115,\n",
       "  'end': 118},\n",
       " {'word': '▁maire',\n",
       "  'score': 0.8729842901229858,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 34,\n",
       "  'start': 118,\n",
       "  'end': 124},\n",
       " {'word': '▁M',\n",
       "  'score': 0.9234569668769836,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 74,\n",
       "  'start': 261,\n",
       "  'end': 263},\n",
       " {'word': '.',\n",
       "  'score': 0.9141836166381836,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 75,\n",
       "  'start': 263,\n",
       "  'end': 264},\n",
       " {'word': '▁de',\n",
       "  'score': 0.9488356113433838,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 76,\n",
       "  'start': 264,\n",
       "  'end': 267},\n",
       " {'word': '▁R',\n",
       "  'score': 0.992038905620575,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 77,\n",
       "  'start': 267,\n",
       "  'end': 269},\n",
       " {'word': 'ê',\n",
       "  'score': 0.9914994239807129,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 78,\n",
       "  'start': 269,\n",
       "  'end': 270},\n",
       " {'word': 'nal',\n",
       "  'score': 0.991144597530365,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 79,\n",
       "  'start': 270,\n",
       "  'end': 273},\n",
       " {'word': '▁Légion',\n",
       "  'score': 0.9943797588348389,\n",
       "  'entity': 'I-ORG',\n",
       "  'index': 96,\n",
       "  'start': 355,\n",
       "  'end': 362},\n",
       " {'word': '▁d',\n",
       "  'score': 0.9943034648895264,\n",
       "  'entity': 'I-ORG',\n",
       "  'index': 97,\n",
       "  'start': 362,\n",
       "  'end': 364},\n",
       " {'word': 'honneur',\n",
       "  'score': 0.9942684173583984,\n",
       "  'entity': 'I-ORG',\n",
       "  'index': 99,\n",
       "  'start': 365,\n",
       "  'end': 372}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e80c1758-bfeb-4d00-8fac-30913c30adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_person_entities(gutenberg_id, grouped_entities=False, max_chunk_len=512, split_chunk_len=256):\n",
    "    '''Given a book ID, returns its text (excluding Project Gutenberg's intro and outro), all its \n",
    "    tokens classified as PER (Person) entities, and all the words corresponding to those tokens, as \n",
    "    well as their index in the sentence and in the book, and their classification score as a PER entity.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    grouped_entities : bool, optional\n",
    "        Flag indicating whether the NER pipeline is configured to outout grouped_entities or not \n",
    "        (default is False)\n",
    "    max_chunk_len : int, optional\n",
    "        Maximum character-level length of each sentence passed to the model (default is 512)\n",
    "    split_chunk_len : int, optional\n",
    "        Maximum character-level length of each sub-sentence passed to the model, when splitting an\n",
    "        overly big sentence into smaller sub-sentences (default is 256)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    book_text : str\n",
    "        The book's text, excluding Project Gutenberg's header and outro\n",
    "    ner_entities_tokens : list\n",
    "        A list containing all the Person tokens found across the whole book\n",
    "    ner_entities_words : list\n",
    "        A list containing dictionary entries of all the Person entities found across the whole book \n",
    "        (the full word corresponding to them (i.e. not separated tokens), their index in the sentence \n",
    "        and in the book overall, and their PER-entity classification score, a number between 0.0 and \n",
    "        1.0)\n",
    "    '''\n",
    "    # code is correct, but gives a warning about the model not having a predefined maximum length,\n",
    "    # suppressing those warnings to not interfere with tdqm progress bar\n",
    "    import warnings\n",
    "    from transformers import logging\n",
    "    warnings.filterwarnings('ignore')\n",
    "    warnings.simplefilter('ignore')\n",
    "    logging.set_verbosity_error()\n",
    "    \n",
    "    # read in gutenberg book\n",
    "    book_text =  get_book_text(gutenberg_id)\n",
    "\n",
    "    # load NER model and tokenizer\n",
    "    ner_model = 'mrm8488/mobilebert-finetuned-ner'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ner_model)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(ner_model)\n",
    "    nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    # prepare for iteration over the book\n",
    "    sentence_level_book = re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', book_text)\n",
    "    ner_entities_tokens = []\n",
    "    ner_entities_words = []\n",
    "    sentence_index = 0\n",
    "    \n",
    "    # iterate over sentence-level chunks\n",
    "    for l in tqdm(sentence_level_book):\n",
    "        l = l.lower()\n",
    "        if len(l) > max_chunk_len:\n",
    "            for m in range(len(l) // split_chunk_len + 1):\n",
    "                new_l = ' '.join(l.split(' ')[m*split_chunk_len:][:(m+1)*split_chunk_len])\n",
    "                ner_entities_tokens, ner_entities_words, sentence_index = get_line_entities(new_l, \n",
    "                                                                                            ner_entities_tokens, \n",
    "                                                                                            ner_entities_words,\n",
    "                                                                                            sentence_index, \n",
    "                                                                                            tokenizer,\n",
    "                                                                                            nlp,\n",
    "                                                                                            grouped_entities)\n",
    "        else:\n",
    "            ner_entities_tokens, ner_entities_words, sentence_index = get_line_entities(l, \n",
    "                                                                                        ner_entities_tokens, \n",
    "                                                                                        ner_entities_words,\n",
    "                                                                                        sentence_index, \n",
    "                                                                                        tokenizer, \n",
    "                                                                                        nlp,\n",
    "                                                                                        grouped_entities)\n",
    "\n",
    "    return book_text, ner_entities_tokens, ner_entities_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "127d49f4-7394-49f5-bd32-d1b5c2c847fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7929/7929 [09:56<00:00, 13.28it/s]\n"
     ]
    }
   ],
   "source": [
    "rouge_noir_id = '798-8'\n",
    "(rouge_noir_text, \n",
    " rouge_noir_ent_tokens, \n",
    " rouge_noir_ent_words) = get_person_entities(rouge_noir_id, grouped_entities=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c9ced57-45a9-44bf-ad51-37c8451d007c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_word\n",
       "julien        1166\n",
       "rênal          595\n",
       "mathilde       340\n",
       "mole           170\n",
       "pirard         106\n",
       "abbé            81\n",
       "croisenois      53\n",
       "sorel           53\n",
       "derville        50\n",
       "norbert         49\n",
       "fervaques       38\n",
       "élisa           29\n",
       "valenod         28\n",
       "napoléon        27\n",
       "altamira        27\n",
       "caylus          27\n",
       "louis           24\n",
       "trouva          23\n",
       "chélan          21\n",
       "amanda          19\n",
       "geronimo        17\n",
       "frilair         16\n",
       "stanislas       16\n",
       "maslon          14\n",
       "don             14\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save entities into a dataframe, and to the disk\n",
    "rouge_noir_df = pd.DataFrame(rouge_noir_ent_words)\n",
    "# view top 25 entities\n",
    "(rouge_noir_df\n",
    " .drop_duplicates('total_word_index')\n",
    " .groupby('full_word')\n",
    " .count()\n",
    " .sort_values(by='score', ascending=False)\n",
    ")['score'][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8979fcf2-cb5b-4859-8a26-d340e4cdec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save entities into a dataframe, and to the disk\n",
    "rouge_noir_df = pd.DataFrame(rouge_noir_ent_words)\n",
    "# rouge_noir_df['full_word'] =  rouge_noir_df['full_word'].apply(lambda s: s.lower())\n",
    "rouge_noir_df.to_csv('../data/book_dfs/rouge_noir_df.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3dd70df7-9ab6-414d-9bee-7821b6dbb6d1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_word\n",
       "julien        450\n",
       "é             369\n",
       "mme           208\n",
       "mathilde       98\n",
       "è              75\n",
       "valenod        70\n",
       "mlle           63\n",
       "mole           54\n",
       "ê              48\n",
       "marquis        43\n",
       "ch             32\n",
       "tait           31\n",
       "sorel          29\n",
       "re             24\n",
       "fouqu          24\n",
       "disait         21\n",
       "tre            19\n",
       "comte          19\n",
       "pr             18\n",
       "ç              17\n",
       "grand          16\n",
       "norbert        16\n",
       "derville       15\n",
       "û              14\n",
       "croisenois     14\n",
       "pensa          14\n",
       "pirard         14\n",
       "res            13\n",
       "chevalier      11\n",
       "nal            11\n",
       "homme          11\n",
       "monsieur       11\n",
       "jour           10\n",
       "mar            10\n",
       "chambre        10\n",
       "napol           9\n",
       "saint           9\n",
       "abb             9\n",
       "tanbeau         8\n",
       "dieu            8\n",
       "esprit          8\n",
       "appert          8\n",
       "maire           7\n",
       "ami             7\n",
       "marquise        7\n",
       "duc             7\n",
       "amour           7\n",
       "caract          7\n",
       "altamira        7\n",
       "voir            7\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view top 25 entities\n",
    "(rouge_noir_df\n",
    " .drop_duplicates('total_word_index')\n",
    " .groupby('full_word')\n",
    " .count()\n",
    " .sort_values(by='score', ascending=False)\n",
    ")['score'][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a309897b-4d20-4394-b03c-5c412948110c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_word\n",
       "julien        1166\n",
       "rênal          595\n",
       "mathilde       340\n",
       "mole           170\n",
       "pirard         106\n",
       "abbé            81\n",
       "croisenois      53\n",
       "sorel           53\n",
       "derville        50\n",
       "norbert         49\n",
       "fervaques       38\n",
       "élisa           29\n",
       "valenod         28\n",
       "napoléon        27\n",
       "altamira        27\n",
       "caylus          27\n",
       "louis           24\n",
       "trouva          23\n",
       "chélan          21\n",
       "amanda          19\n",
       "geronimo        17\n",
       "frilair         16\n",
       "stanislas       16\n",
       "maslon          14\n",
       "don             14\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save entities into a dataframe, and to the disk\n",
    "rouge_noir_df = pd.DataFrame(rouge_noir_ent_words)\n",
    "# rouge_noir_df['full_word'] =  rouge_noir_df['full_word'].apply(lambda s: s.lower())\n",
    "rouge_noir_df.to_csv('../data/book_dfs/798-8.csv', index=False) \n",
    "\n",
    "# view top 25 entities\n",
    "(rouge_noir_df\n",
    " .drop_duplicates('total_word_index')\n",
    " .groupby('full_word')\n",
    " .count()\n",
    " .sort_values(by='score', ascending=False)\n",
    ")['score'][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b2563a6-7f48-4403-a61d-142df93671cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b3cf6d-c119-4dd6-bd89-8f8219d2b822",
   "metadata": {},
   "source": [
    "### Fine-tuned BERT embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13d62bf-97ca-4b49-8b7a-f36c46d29f21",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Fine-tune the (BERT) model\n",
    "\n",
    "The following cells fine-tune the BERT Large Cased pre-trained model on a book. Indeed, the idea behind the fine-tuning is exactly to conduct it for the book which will later be analyzed using the later generated embeddings. To achieve this, a dummy task is used -- for this training, the task was identifying whether, given a random sentence of the book, it contained at least one (NER-identified) character in it.\n",
    "\n",
    "The fine-tuning below is conducted for Charles Dicken's *David Copperfield* book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22348c81-deac-48d8-ac64-4e47f5040350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_df(gutenberg_id, grouped_entities=False):\n",
    "    '''Creates an Entities dataframe for the given book. If it already exists, just loads it from memory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    grouped_entities : bool, optional\n",
    "        Flag indicating whether the NER pipeline used to create the entities was configured to output \n",
    "        grouped_entities or not (default is False)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    book_df : DataFrame\n",
    "        A DataFrame containing the book's entities, their sentence-wise and book-wise indexes and their\n",
    "        PER-classification scores\n",
    "    '''\n",
    "    \n",
    "    # check if df already exists on the disk\n",
    "    book_csv_path = f'data/book_dfs/{gutenberg_id}.csv'\n",
    "    if grouped_entities:\n",
    "        book_csv_path = f'data/book_dfs/{gutenberg_id}_grouped.csv'\n",
    "    if os.path.isfile(book_csv_path):\n",
    "        return pd.read_csv(book_csv_path)\n",
    "    \n",
    "    # if df doesn't already exist, create it\n",
    "    (book_text, book_ent_tokens, book_ent_words) = get_person_entities(gutenberg_id, \n",
    "                                                                       grouped_entities=grouped_entities)\n",
    "    book_df = pd.DataFrame(book_ent_words)\n",
    "    book_df['full_word'] =  book_df['full_word'].apply(lambda s: s.lower())\n",
    "    \n",
    "    # save df to disk and then return it\n",
    "    book_df.to_csv(book_csv_path, index=False) \n",
    "    return book_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39b052b1-d551-4d70-a26a-93d0fc76c543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T06:22:02.387969Z",
     "start_time": "2021-05-12T06:21:45.153028Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7929/7929 [09:56<00:00, 13.28it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/book_dfs/798-8.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-cd97f474dfc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbook_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_book_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'798-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbook_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_book_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'798-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentence_level_book\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'(?<=[^A-Z].[.?]) +(?=[A-Z])'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-5bf40cd1c199>\u001b[0m in \u001b[0;36mget_book_df\u001b[0;34m(gutenberg_id, grouped_entities)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# save df to disk and then return it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mbook_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook_csv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbook_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3385\u001b[0m         )\n\u001b[1;32m   3386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3387\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3388\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3389\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         )\n\u001b[0;32m-> 1083\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \"\"\"\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/book_dfs/798-8.csv'"
     ]
    }
   ],
   "source": [
    "book_df = get_book_df('798-8')\n",
    "book_text = get_book_text('798-8')\n",
    "sentence_level_book = re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', book_text)\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentence_level_book]\n",
    "labels = [1 if any([n in s for n in book_df['full_word'].unique()]) else 0 for s in sentences]\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "input_ids=[]\n",
    "for i in tqdm(range(len(tokenized_texts))):\n",
    "    input_ids.append(tokenizer.convert_tokens_to_ids(tokenized_texts[i]))\n",
    "\n",
    "MAX_LEN = 512\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136aeac6-6f91-4a5a-b906-0ae9b6506bde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T06:22:04.874423Z",
     "start_time": "2021-05-12T06:22:02.389916Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate the attention masks\n",
    "attention_masks = []\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "# creating the training and validation sets\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,random_state=56, test_size=0.2)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,random_state=56, test_size=0.2)\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# create an iterator of both our training and validation data with torch DataLoader\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e8be02-8a18-4a23-9174-9a60b8997dc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T06:22:20.691328Z",
     "start_time": "2021-05-12T06:22:04.876204Z"
    },
    "collapsed": true,
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = BertConfig.from_pretrained('bert-large-cased', output_hidden_states=True, num_labels=2)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-large-cased', config=config)\n",
    "model.to(device) \n",
    "\n",
    "lr = 2e-5\n",
    "max_grad_norm = 1.0\n",
    "num_total_steps = 1000\n",
    "num_warmup_steps = 100\n",
    "warmup_proportion = float(num_warmup_steps) / float(num_total_steps)  # 0.1\n",
    "\n",
    "# to reproduce BertAdam specific behavior set correct_bias=False\n",
    "optimizer = AdamW(model.parameters(), lr=lr)  \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_total_steps)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e15905-ad98-4d05-9665-c574276811b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T08:16:35.927818Z",
     "start_time": "2021-05-12T06:22:20.693814Z"
    },
    "collapsed": true,
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "total_step = len(train_dataloader)\n",
    "train_loss_set = []\n",
    "epochs = 2\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    # Train the data for one epoch\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        train_loss_set.append(loss.item())    \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        if (i) % 50 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45cb3d9-b498-4141-be6f-b2cc81e70b87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T08:16:40.534048Z",
     "start_time": "2021-05-12T08:16:35.929743Z"
    },
    "collapsed": true,
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'model_finetuning/BERT/798-8/checkpoints/finetuned_bert-large-cased.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41678b0a-6888-46c9-9a0e-504f91067527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
