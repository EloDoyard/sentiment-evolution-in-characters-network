{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6abdc6b2-0a30-47ea-9f26-695f227086e1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-LOC',\n",
       "  'score': 0.55669767,\n",
       "  'index': 10,\n",
       "  'word': '▁d',\n",
       "  'start': 36,\n",
       "  'end': 38},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.95578015,\n",
       "  'index': 12,\n",
       "  'word': 'Italie',\n",
       "  'start': 39,\n",
       "  'end': 45},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9981791,\n",
       "  'index': 16,\n",
       "  'word': '▁V',\n",
       "  'start': 55,\n",
       "  'end': 57},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9979278,\n",
       "  'index': 17,\n",
       "  'word': 'errière',\n",
       "  'start': 57,\n",
       "  'end': 64},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99829894,\n",
       "  'index': 18,\n",
       "  'word': 's',\n",
       "  'start': 64,\n",
       "  'end': 65},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9855219,\n",
       "  'index': 31,\n",
       "  'word': '▁M',\n",
       "  'start': 112,\n",
       "  'end': 114},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9845025,\n",
       "  'index': 32,\n",
       "  'word': '.',\n",
       "  'start': 114,\n",
       "  'end': 115},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.8523049,\n",
       "  'index': 33,\n",
       "  'word': '▁le',\n",
       "  'start': 115,\n",
       "  'end': 118},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.87298524,\n",
       "  'index': 34,\n",
       "  'word': '▁maire',\n",
       "  'start': 118,\n",
       "  'end': 124},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.92345726,\n",
       "  'index': 74,\n",
       "  'word': '▁M',\n",
       "  'start': 261,\n",
       "  'end': 263},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.91418386,\n",
       "  'index': 75,\n",
       "  'word': '.',\n",
       "  'start': 263,\n",
       "  'end': 264},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9488357,\n",
       "  'index': 76,\n",
       "  'word': '▁de',\n",
       "  'start': 264,\n",
       "  'end': 267},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99203897,\n",
       "  'index': 77,\n",
       "  'word': '▁R',\n",
       "  'start': 267,\n",
       "  'end': 269},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99149936,\n",
       "  'index': 78,\n",
       "  'word': 'ê',\n",
       "  'start': 269,\n",
       "  'end': 270},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9911446,\n",
       "  'index': 79,\n",
       "  'word': 'nal',\n",
       "  'start': 270,\n",
       "  'end': 273},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9943797,\n",
       "  'index': 96,\n",
       "  'word': '▁Légion',\n",
       "  'start': 355,\n",
       "  'end': 362},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9943036,\n",
       "  'index': 97,\n",
       "  'word': '▁d',\n",
       "  'start': 362,\n",
       "  'end': 364},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9942684,\n",
       "  'index': 99,\n",
       "  'word': 'honneur',\n",
       "  'start': 365,\n",
       "  'end': 372}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9440d04f-a0df-412f-9d56-00b2628bf528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "book_text =  get_book_text('798-8')\n",
    "ner_model = 'mrm8488/mobilebert-finetuned-ner'\n",
    "tokenizer = AutoTokenizer.from_pretrained(ner_model)\n",
    "model = AutoModelForTokenClassification.from_pretrained(ner_model)\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "sentence_level_book = re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', book_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "155876e1-9216-4731-bfd5-a0db0b79e9be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new entity tokens\n",
      "[{'entity': 'I-PER', 'score': 0.9980604, 'index': 42, 'word': 'm', 'start': 113, 'end': 114}, {'entity': 'I-PER', 'score': 0.9974752, 'index': 44, 'word': 'le', 'start': 116, 'end': 118}, {'entity': 'I-PER', 'score': 0.93767214, 'index': 45, 'word': 'mai', 'start': 119, 'end': 122}, {'entity': 'I-PER', 'score': 0.906227, 'index': 46, 'word': '##re', 'start': 122, 'end': 124}, {'entity': 'I-PER', 'score': 0.9871251, 'index': 48, 'word': 'jacob', 'start': 126, 'end': 131}, {'entity': 'I-PER', 'score': 0.990449, 'index': 49, 'word': '##in', 'start': 131, 'end': 133}, {'entity': 'I-PER', 'score': 0.955765, 'index': 51, 'word': 'bon', 'start': 137, 'end': 140}, {'entity': 'I-PER', 'score': 0.8932861, 'index': 54, 'word': '##ste', 'start': 146, 'end': 149}, {'entity': 'I-PER', 'score': 0.9953166, 'index': 97, 'word': 'm', 'start': 262, 'end': 263}, {'entity': 'I-PER', 'score': 0.9966055, 'index': 99, 'word': 'de', 'start': 265, 'end': 267}, {'entity': 'I-PER', 'score': 0.9961195, 'index': 100, 'word': 'renal', 'start': 268, 'end': 273}]\n",
      "line words\n",
      "['Un', 'vieux', 'chirurgien', '-', 'major', 'de', 'l', \"'\", 'armée', 'd', \"'\", 'Italie', ',', 'retiré', 'à', 'Verrières', ',', 'et', 'qui', 'de', 'son', 'vivant', 'était', 'à', 'la', 'fois', ',', 'suivant', 'M', '.', 'le', 'maire', ',', 'jacobin', 'et', 'bonapartiste', ',', 'osa', 'bien', 'un', 'jour', 'se', 'plaindre', 'à', 'lui', 'de', 'la', 'mutilation', 'périodique', 'de', 'ces', 'beaux', 'arbres', '.', '-', '-', 'J', \"'\", 'aime', 'l', \"'\", 'ombre', ',', 'répondit', 'M', '.', 'de', 'Rênal', 'avec', 'la', 'nuance', 'de', 'hauteur', 'convenable', 'quand', 'on', 'parle', 'à', 'un', 'chirurgien', ',', 'membre', 'de', 'la', 'Légion', 'd', \"'\", 'honneur', ',', 'j', \"'\", 'aime', 'l', \"'\", 'ombre', ',', 'je', 'fais', 'tailler', 'mes', 'arbres', 'pour', 'donner', 'de', 'l', \"'\", 'ombre', ',', 'et', 'je', 'ne', 'conçois', 'pas', 'qu', \"'\", 'un', 'arbre', 'soit', 'fait', 'pour', 'autre', 'chose', ',', 'quand', 'toutefois', ',', 'comme', 'l', \"'\", 'utile', 'noyer', ',', 'il', '_', 'ne', 'rapporte', 'pas', 'de', 'revenu', '_', '.']\n"
     ]
    }
   ],
   "source": [
    "l=sentence_level_book[65]\n",
    "a,b,c = get_line_entities(l, [], [], 0, tokenizer, nlp,\n",
    "                     False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f606ee3d-4bba-457d-ba8d-85c41ae0526b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': '▁d',\n",
       "  'score': 0.5566989183425903,\n",
       "  'entity': 'I-LOC',\n",
       "  'index': 10,\n",
       "  'start': 36,\n",
       "  'end': 38},\n",
       " {'word': 'Italie',\n",
       "  'score': 0.9557802677154541,\n",
       "  'entity': 'I-LOC',\n",
       "  'index': 12,\n",
       "  'start': 39,\n",
       "  'end': 45},\n",
       " {'word': '▁V',\n",
       "  'score': 0.998179018497467,\n",
       "  'entity': 'I-LOC',\n",
       "  'index': 16,\n",
       "  'start': 55,\n",
       "  'end': 57},\n",
       " {'word': 'errière',\n",
       "  'score': 0.9979278445243835,\n",
       "  'entity': 'I-LOC',\n",
       "  'index': 17,\n",
       "  'start': 57,\n",
       "  'end': 64},\n",
       " {'word': 's',\n",
       "  'score': 0.9982990026473999,\n",
       "  'entity': 'I-LOC',\n",
       "  'index': 18,\n",
       "  'start': 64,\n",
       "  'end': 65},\n",
       " {'word': '▁M',\n",
       "  'score': 0.9855217337608337,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 31,\n",
       "  'start': 112,\n",
       "  'end': 114},\n",
       " {'word': '.',\n",
       "  'score': 0.9845024347305298,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 32,\n",
       "  'start': 114,\n",
       "  'end': 115},\n",
       " {'word': '▁le',\n",
       "  'score': 0.8523029685020447,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 33,\n",
       "  'start': 115,\n",
       "  'end': 118},\n",
       " {'word': '▁maire',\n",
       "  'score': 0.8729842901229858,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 34,\n",
       "  'start': 118,\n",
       "  'end': 124},\n",
       " {'word': '▁M',\n",
       "  'score': 0.9234569668769836,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 74,\n",
       "  'start': 261,\n",
       "  'end': 263},\n",
       " {'word': '.',\n",
       "  'score': 0.9141836166381836,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 75,\n",
       "  'start': 263,\n",
       "  'end': 264},\n",
       " {'word': '▁de',\n",
       "  'score': 0.9488356113433838,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 76,\n",
       "  'start': 264,\n",
       "  'end': 267},\n",
       " {'word': '▁R',\n",
       "  'score': 0.992038905620575,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 77,\n",
       "  'start': 267,\n",
       "  'end': 269},\n",
       " {'word': 'ê',\n",
       "  'score': 0.9914994239807129,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 78,\n",
       "  'start': 269,\n",
       "  'end': 270},\n",
       " {'word': 'nal',\n",
       "  'score': 0.991144597530365,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 79,\n",
       "  'start': 270,\n",
       "  'end': 273},\n",
       " {'word': '▁Légion',\n",
       "  'score': 0.9943797588348389,\n",
       "  'entity': 'I-ORG',\n",
       "  'index': 96,\n",
       "  'start': 355,\n",
       "  'end': 362},\n",
       " {'word': '▁d',\n",
       "  'score': 0.9943034648895264,\n",
       "  'entity': 'I-ORG',\n",
       "  'index': 97,\n",
       "  'start': 362,\n",
       "  'end': 364},\n",
       " {'word': 'honneur',\n",
       "  'score': 0.9942684173583984,\n",
       "  'entity': 'I-ORG',\n",
       "  'index': 99,\n",
       "  'start': 365,\n",
       "  'end': 372}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "127d49f4-7394-49f5-bd32-d1b5c2c847fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7929/7929 [09:56<00:00, 13.28it/s]\n"
     ]
    }
   ],
   "source": [
    "rouge_noir_id = '798-8'\n",
    "(rouge_noir_text, \n",
    " rouge_noir_ent_tokens, \n",
    " rouge_noir_ent_words) = get_person_entities(rouge_noir_id, grouped_entities=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c9ced57-45a9-44bf-ad51-37c8451d007c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_word\n",
       "julien        1166\n",
       "rênal          595\n",
       "mathilde       340\n",
       "mole           170\n",
       "pirard         106\n",
       "abbé            81\n",
       "croisenois      53\n",
       "sorel           53\n",
       "derville        50\n",
       "norbert         49\n",
       "fervaques       38\n",
       "élisa           29\n",
       "valenod         28\n",
       "napoléon        27\n",
       "altamira        27\n",
       "caylus          27\n",
       "louis           24\n",
       "trouva          23\n",
       "chélan          21\n",
       "amanda          19\n",
       "geronimo        17\n",
       "frilair         16\n",
       "stanislas       16\n",
       "maslon          14\n",
       "don             14\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save entities into a dataframe, and to the disk\n",
    "rouge_noir_df = pd.DataFrame(rouge_noir_ent_words)\n",
    "# view top 25 entities\n",
    "(rouge_noir_df\n",
    " .drop_duplicates('total_word_index')\n",
    " .groupby('full_word')\n",
    " .count()\n",
    " .sort_values(by='score', ascending=False)\n",
    ")['score'][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8979fcf2-cb5b-4859-8a26-d340e4cdec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save entities into a dataframe, and to the disk\n",
    "rouge_noir_df = pd.DataFrame(rouge_noir_ent_words)\n",
    "# rouge_noir_df['full_word'] =  rouge_noir_df['full_word'].apply(lambda s: s.lower())\n",
    "rouge_noir_df.to_csv('../data/book_dfs/rouge_noir_df.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3dd70df7-9ab6-414d-9bee-7821b6dbb6d1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_word\n",
       "julien        450\n",
       "é             369\n",
       "mme           208\n",
       "mathilde       98\n",
       "è              75\n",
       "valenod        70\n",
       "mlle           63\n",
       "mole           54\n",
       "ê              48\n",
       "marquis        43\n",
       "ch             32\n",
       "tait           31\n",
       "sorel          29\n",
       "re             24\n",
       "fouqu          24\n",
       "disait         21\n",
       "tre            19\n",
       "comte          19\n",
       "pr             18\n",
       "ç              17\n",
       "grand          16\n",
       "norbert        16\n",
       "derville       15\n",
       "û              14\n",
       "croisenois     14\n",
       "pensa          14\n",
       "pirard         14\n",
       "res            13\n",
       "chevalier      11\n",
       "nal            11\n",
       "homme          11\n",
       "monsieur       11\n",
       "jour           10\n",
       "mar            10\n",
       "chambre        10\n",
       "napol           9\n",
       "saint           9\n",
       "abb             9\n",
       "tanbeau         8\n",
       "dieu            8\n",
       "esprit          8\n",
       "appert          8\n",
       "maire           7\n",
       "ami             7\n",
       "marquise        7\n",
       "duc             7\n",
       "amour           7\n",
       "caract          7\n",
       "altamira        7\n",
       "voir            7\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view top 25 entities\n",
    "(rouge_noir_df\n",
    " .drop_duplicates('total_word_index')\n",
    " .groupby('full_word')\n",
    " .count()\n",
    " .sort_values(by='score', ascending=False)\n",
    ")['score'][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a309897b-4d20-4394-b03c-5c412948110c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_word\n",
       "julien        1166\n",
       "rênal          595\n",
       "mathilde       340\n",
       "mole           170\n",
       "pirard         106\n",
       "abbé            81\n",
       "croisenois      53\n",
       "sorel           53\n",
       "derville        50\n",
       "norbert         49\n",
       "fervaques       38\n",
       "élisa           29\n",
       "valenod         28\n",
       "napoléon        27\n",
       "altamira        27\n",
       "caylus          27\n",
       "louis           24\n",
       "trouva          23\n",
       "chélan          21\n",
       "amanda          19\n",
       "geronimo        17\n",
       "frilair         16\n",
       "stanislas       16\n",
       "maslon          14\n",
       "don             14\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save entities into a dataframe, and to the disk\n",
    "rouge_noir_df = pd.DataFrame(rouge_noir_ent_words)\n",
    "# rouge_noir_df['full_word'] =  rouge_noir_df['full_word'].apply(lambda s: s.lower())\n",
    "rouge_noir_df.to_csv('../data/book_dfs/798-8.csv', index=False) \n",
    "\n",
    "# view top 25 entities\n",
    "(rouge_noir_df\n",
    " .drop_duplicates('total_word_index')\n",
    " .groupby('full_word')\n",
    " .count()\n",
    " .sort_values(by='score', ascending=False)\n",
    ")['score'][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b2563a6-7f48-4403-a61d-142df93671cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fe66d50-0bbb-4aed-9fe9-18d84468b45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../src')\n",
    "\n",
    "from character_extraction import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63934667-ffab-4079-bb72-cc7c986f5c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_text =  get_book_text('798-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e474add9-6586-48ea-bda9-67893d695b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_pg_id = '798-8'\n",
    "book_luke_checkpoints_tmp_dir = f'../data/book_dfs/tmp/{book_pg_id}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca20dbc-ee67-4b19-9b45-3bf1432dda3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7929 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "continue_from_ckpt = False\n",
    "\n",
    "if not continue_from_ckpt:\n",
    "    book_text, ner_entities_words = get_LUKE_person_entities(book_pg_id, \n",
    "                                                             book_luke_checkpoints_tmp_dir, \n",
    "                                                             ner_entities_words = [])\n",
    "    \n",
    "else:\n",
    "    # continue from checkpoint, if needed\n",
    "    latest_checkpoint = 10500\n",
    "    luke_df = pd.read_csv(f'{book_luke_checkpoints_tmp_dir}luke_tmp_df_{latest_checkpoint:05d}.csv')\n",
    "    ner_entities_checkpoint = [dict(zip(['full_word', 'sentence_word_index', 'total_word_index'], v)) \n",
    "                               for v in [tuple(r) for r in luke_df.to_numpy()]]\n",
    "\n",
    "    book_text, ner_entities_words = get_LUKE_person_entities(book_pg_id, \n",
    "                                                             book_luke_checkpoints_tmp_dir,\n",
    "                                                             last_checkpoint = latest_checkpoint,\n",
    "                                                             ner_entities_words = ner_entities_checkpoint)\n",
    "    \n",
    "luke_df = pd.DataFrame(ner_entities_words)\n",
    "luke_df.to_csv(f'notebooks_data/book_dfs/luke_{book_pg_id}_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "929c37fe-03cc-461a-9b2f-641f1a042637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7929/7929 [10:07<00:00, 13.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "full_word\n",
       "julien        1166\n",
       "rênal          595\n",
       "mathilde       340\n",
       "mole           170\n",
       "pirard         106\n",
       "abbé            81\n",
       "croisenois      53\n",
       "sorel           53\n",
       "derville        50\n",
       "norbert         49\n",
       "fervaques       38\n",
       "élisa           29\n",
       "valenod         28\n",
       "napoléon        27\n",
       "altamira        27\n",
       "caylus          27\n",
       "louis           24\n",
       "trouva          23\n",
       "chélan          21\n",
       "amanda          19\n",
       "geronimo        17\n",
       "frilair         16\n",
       "stanislas       16\n",
       "maslon          14\n",
       "don             14\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_noir_id = '798-8'\n",
    "(rouge_noir_text, \n",
    " rouge_noir_ent_tokens, \n",
    " rouge_noir_ent_words) = get_person_entities(rouge_noir_id, grouped_entities=False)\n",
    "\n",
    "# save entities into a dataframe, and to the disk\n",
    "red_black_df = pd.DataFrame(rouge_noir_ent_words)\n",
    "# rouge_noir_df['full_word'] =  rouge_noir_df['full_word'].apply(lambda s: s.lower())\n",
    "red_black_df.to_csv('../data/book_dfs/red_black_df.csv', index=False) \n",
    "\n",
    "# view top 25 entities\n",
    "(red_black_df\n",
    " .drop_duplicates('total_word_index')\n",
    " .groupby('full_word')\n",
    " .count()\n",
    " .sort_values(by='score', ascending=False)\n",
    ")['score'][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e4c128d-2182-4d5d-a5d8-8fc06cadf56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from character_extraction import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbdd14cd-40c0-498b-8444-ca7dbdaecc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/7929 [00:00<10:21, 12.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8c61ea6aa29b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m (rouge_noir_text, \n\u001b[1;32m      3\u001b[0m  \u001b[0mrouge_noir_ent_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m  rouge_noir_ent_words) = get_person_entities(rouge_noir_id, grouped_entities=True)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# save entities into a dataframe, and to the disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/MA-4/Projet de Semestre/github/sentiment-evolution-in-characters-network/src/character_extraction.py\u001b[0m in \u001b[0;36mget_person_entities\u001b[0;34m(gutenberg_id, grouped_entities, max_chunk_len, split_chunk_len)\u001b[0m\n\u001b[1;32m    179\u001b[0m                                                                                             \u001b[0mner_entities_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                                                                                             \u001b[0mner_entities_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                                                                                             \u001b[0msentence_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                                                                                             \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                                                                                             \u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/MA-4/Projet de Semestre/github/sentiment-evolution-in-characters-network/src/character_extraction.py\u001b[0m in \u001b[0;36mget_line_entities\u001b[0;34m(l, ner_entities_tokens, ner_entities_words, sentence_index, tokenizer, nlp, grouped_entities)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mnew_entity_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgrouped_entities\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mnew_entity_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'PER'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entity_group'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mnew_entity_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'PER'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/pipelines/token_classification.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"offset_mapping\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffset_mapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset_mapping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    941\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/pipelines/token_classification.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         return {\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1560\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1562\u001b[0;31m         outputs = self.mobilebert(\n\u001b[0m\u001b[1;32m   1563\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_hidden_states, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         )\n\u001b[0;32m--> 874\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    875\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    553\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    556\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         outputs = (\n\u001b[1;32m    519\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, intermediate_states, residual_tensor_1, residual_tensor_2)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresidual_tensor_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbottleneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual_tensor_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, residual_tensor)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_outputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresidual_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rouge_noir_id = '798-8'\n",
    "(rouge_noir_text, \n",
    " rouge_noir_ent_tokens, \n",
    " rouge_noir_ent_words) = get_person_entities(rouge_noir_id, grouped_entities=True)\n",
    "\n",
    "# save entities into a dataframe, and to the disk\n",
    "rouge_noir_df = pd.DataFrame(rouge_noir_ent_words)\n",
    "# rouge_noir_df['full_word'] =  rouge_noir_df['full_word'].apply(lambda s: s.lower())\n",
    "rouge_noir_df.to_csv('../data/book_dfs/798-8.csv', index=False) \n",
    "\n",
    "# view top 25 entities\n",
    "(rouge_noir_df\n",
    " .drop_duplicates('total_word_index')\n",
    " .groupby('full_word')\n",
    " .count()\n",
    " .sort_values(by='score', ascending=False)\n",
    ")['score'][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590c063e-fa98-4af6-9b72-f37c664b69cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
