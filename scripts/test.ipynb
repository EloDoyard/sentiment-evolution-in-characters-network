{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2057c890-73f3-4c01-9762-5dba08d2221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/eloisedoyard/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/eloisedoyard/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/eloisedoyard/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/eloisedoyard/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/eloisedoyard/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from transformers import BertConfig, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, \\\n",
    "                         AutoTokenizer, AutoModelForTokenClassification, pipeline, BertTokenizer, BertModel, \\\n",
    "                         LukeTokenizer, LukeForEntitySpanClassification\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ba66e2-6043-463f-acb8-3f59d30ce3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_text(gutenberg_id):\n",
    "    '''Given a book ID, returns the book's text, excluding Project Gutenberg's header and outro.\n",
    "    \n",
    "     Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    book_text : str\n",
    "        The book's text, excluding Project Gutenberg's header and outro\n",
    "    '''\n",
    "    \n",
    "    context = ''\n",
    "    with open(f'../data/book/PG-{gutenberg_id}.txt', mode='r', encoding='utf-8') as f:\n",
    "        context = f.read()\n",
    "    return ' '.join([l for l in (context.split('End of the Project Gutenberg EBook of ')[0]\n",
    "                                        .split('*** END OF THE PROJECT GUTENBERG EBOOK')[0]\n",
    "                                        .split('\\n')) if l][16:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d91f53a-fc03-400f-a514-4ce6d59359d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertModel, CamembertTokenizer\n",
    "from transformers import FlaubertTokenizer, FlaubertModel, FlaubertForTokenClassification\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6477ccb4-09a1-46e9-b628-e6d4f4434155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_entities(l, ner_entities_tokens, ner_entities_words, sentence_index, tokenizer, nlp,\n",
    "                     grouped_entities):\n",
    "    '''Given a line, lists for tokens and words, and word index at the end of the sentence, as well as\n",
    "    the tokenizer and nlp model instances (from huggingface's transformers), updates the tokens and\n",
    "    words lists and the word index to include the given line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    l : str\n",
    "        The line to analyze\n",
    "    ner_entities_tokens : list\n",
    "        A list containing all the Person tokens found so far, across all the previous lines\n",
    "    ner_entities_words : list\n",
    "        A list containing dictionary entries of all the Person entities found so far (the full \n",
    "        word corresponding to them (i.e. not separated tokens), their index in the sentence and \n",
    "        in the book overall, and their PER-entity classification score, a number between 0.0 and \n",
    "        1.0), across all the previous lines\n",
    "    sentence_index : int\n",
    "        The overall (book-wise) index of the first word of the sentence\n",
    "    tokenizer : AutoTokenizer\n",
    "        huggingface's tokenizer being used in the NER pipeline\n",
    "    nlp : pipeline\n",
    "        huggingface's NER pipeline object\n",
    "    grouped_entities : bool\n",
    "        Flag indicating whether the NER pipeline is configured to output grouped_entities or not\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ner_entities_tokens : list\n",
    "        A list containing all the Person tokens found so far, across this and all the previous lines\n",
    "    ner_entities_words : list\n",
    "        A list containing dictionary entries of all the Person entities found so far (the full \n",
    "        word corresponding to them (i.e. not separated tokens), their index in the sentence and \n",
    "        in the book overall, and their PER-entity classification score, a number between 0.0 and \n",
    "        1.0), across this and all the previous lines\n",
    "    sentence_index : int\n",
    "        The overall (book-wise) index of the first word of the next sentence\n",
    "    '''\n",
    "    new_entity_tokens = []\n",
    "    if grouped_entities:\n",
    "        new_entity_tokens = [e for e in nlp(l) if 'PER' in e['entity_group']]\n",
    "    else:\n",
    "        new_entity_tokens = [e for e in nlp(l) if 'PER' in e['entity']]\n",
    "    ner_entities_tokens += new_entity_tokens\n",
    "    \n",
    "    tokenized_line = tokenizer(l)\n",
    "    line_words = [w if w != 'word_tokenize_splits_cannot_into_2_words' else 'cannot'\n",
    "                    for w in word_tokenize(\n",
    "                                           re.sub(r'[^a-zA-Z0-9]', ' \\g<0> ', \n",
    "                                                  l).replace('cannot', \n",
    "                                                            'word_tokenize_splits_cannot_into_2_words'))]\n",
    "    \n",
    "    # go from token to word with\n",
    "    for et in new_entity_tokens:\n",
    "        if grouped_entities:\n",
    "            # find index of grouped entity\n",
    "            reconstructed_line = ' '.join([lw.lower() for lw in line_words])\n",
    "            first_word = word_tokenize(re.sub(r'[^a-zA-Z0-9]', ' \\g<0> ', et['word']))[0]\n",
    "            if et['word'][0] == '#':\n",
    "                first_word = word_tokenize(re.sub(r'[^a-zA-Z0-9]', ' \\g<0> ', et['word'][2:]))[0]\n",
    "            \n",
    "            word_index = len(reconstructed_line[:reconstructed_line.index(first_word)].split())\n",
    "\n",
    "            if et['word'] not in stopwords.words('french') and et['word'].isalpha():\n",
    "                # record grouped entity\n",
    "                ner_entities_words += [{'full_word': et['word'], \n",
    "                                        'sentence_word_index': word_index, \n",
    "                                        'total_word_index': sentence_index+word_index,\n",
    "                                        'score': et['score']}]\n",
    "        else:\n",
    "            # record non-grouped entity\n",
    "            word_index = tokenized_line.word_ids()[et['index']]\n",
    "            if line_words[word_index] not in stopwords.words('french') and line_words[word_index].isalpha():\n",
    "                ner_entities_words += [{'full_word': line_words[word_index], \n",
    "                                        'sentence_word_index': word_index, \n",
    "                                        'total_word_index': sentence_index+word_index,\n",
    "                                        'score': et['score']}]\n",
    "    sentence_index += len(line_words)\n",
    "    return ner_entities_tokens, ner_entities_words, sentence_index"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47ed33fd-2394-4085-ae8c-791aea30604a",
   "metadata": {},
   "source": [
    "get_line_entities(sentence_level_book[65], ner_entities_tokens, ner_entities_words,sentence_index, tokenizer, nlp, False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b2ee4d0-75b3-4c58-9820-fafb725cf550",
   "metadata": {},
   "source": [
    "# read in gutenberg book\n",
    "book_text =  get_book_text('798-8')\n",
    "\n",
    "# load NER model and tokenizer\n",
    "ner_model = 'Jean-Baptiste/camembert-ner'\n",
    "tokenizer = AutoTokenizer.from_pretrained(ner_model)\n",
    "model = AutoModelForTokenClassification.from_pretrained(ner_model)\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# prepare for iteration over the book\n",
    "sentence_level_book = re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', book_text)\n",
    "ner_entities_tokens = []\n",
    "ner_entities_words = []\n",
    "sentence_index = 0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c6bb8ce-2fae-4cb6-a6e2-51a747cb7000",
   "metadata": {},
   "source": [
    "grouped_entities = False\n",
    "l = sentence_level_book[65]\n",
    "new_entity_tokens = []\n",
    "if grouped_entities:\n",
    "    new_entity_tokens = [e for e in nlp(l) if 'PER' in e['entity_group']]\n",
    "else:\n",
    "    new_entity_tokens = [e for e in nlp(l) if 'PER' in e['entity']]\n",
    "ner_entities_tokens += new_entity_tokens\n",
    "print (ner_entities_tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cdf2a426-3076-4bb5-b09f-12a8d4932cba",
   "metadata": {},
   "source": [
    "tokenized_line = tokenizer(l)\n",
    "line_words = [w if w != 'word_tokenize_splits_cannot_into_2_words' else 'cannot'\n",
    "                for w in word_tokenize(\n",
    "                                       re.sub(r'[^a-zA-Z0-9]', ' \\g<0> ', \n",
    "                                              l).replace('cannot', \n",
    "                                                        'word_tokenize_splits_cannot_into_2_words'))]\n",
    "\n",
    "# go from token to word with\n",
    "for et in new_entity_tokens:\n",
    "    if grouped_entities:\n",
    "        # find index of grouped entity\n",
    "        reconstructed_line = ' '.join([lw.lower() for lw in line_words])\n",
    "        first_word = word_tokenize(re.sub(r'[^a-zA-Z0-9]', ' \\g<0> ', et['word']))[0]\n",
    "        if et['word'][0] == '#':\n",
    "            first_word = word_tokenize(re.sub(r'[^a-zA-Z0-9]', ' \\g<0> ', et['word'][2:]))[0]\n",
    "\n",
    "        word_index = len(reconstructed_line[:reconstructed_line.index(first_word)].split())\n",
    "\n",
    "        if et['word'] not in stopwords.words('french') and et['word'].isalpha():\n",
    "            # record grouped entity\n",
    "            ner_entities_words += [{'full_word': et['word'], \n",
    "                                    'sentence_word_index': word_index, \n",
    "                                    'total_word_index': sentence_index+word_index,\n",
    "                                    'score': et['score']}]\n",
    "    else:\n",
    "        # record non-grouped entity\n",
    "        word_index = tokenized_line.word_ids()[et['index']]\n",
    "        if line_words[word_index] not in stopwords.words('french') and line_words[word_index].isalpha():\n",
    "            ner_entities_words += [{'full_word': line_words[word_index], \n",
    "                                    'sentence_word_index': word_index, \n",
    "                                    'total_word_index': sentence_index+word_index,\n",
    "                                    'score': et['score']}]\n",
    "sentence_index += len(line_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e80c1758-bfeb-4d00-8fac-30913c30adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_person_entities(gutenberg_id, grouped_entities=False, max_chunk_len=512, split_chunk_len=256):\n",
    "    '''Given a book ID, returns its text (excluding Project Gutenberg's intro and outro), all its \n",
    "    tokens classified as PER (Person) entities, and all the words corresponding to those tokens, as \n",
    "    well as their index in the sentence and in the book, and their classification score as a PER entity.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    grouped_entities : bool, optional\n",
    "        Flag indicating whether the NER pipeline is configured to outout grouped_entities or not \n",
    "        (default is False)\n",
    "    max_chunk_len : int, optional\n",
    "        Maximum character-level length of each sentence passed to the model (default is 512)\n",
    "    split_chunk_len : int, optional\n",
    "        Maximum character-level length of each sub-sentence passed to the model, when splitting an\n",
    "        overly big sentence into smaller sub-sentences (default is 256)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    book_text : str\n",
    "        The book's text, excluding Project Gutenberg's header and outro\n",
    "    ner_entities_tokens : list\n",
    "        A list containing all the Person tokens found across the whole book\n",
    "    ner_entities_words : list\n",
    "        A list containing dictionary entries of all the Person entities found across the whole book \n",
    "        (the full word corresponding to them (i.e. not separated tokens), their index in the sentence \n",
    "        and in the book overall, and their PER-entity classification score, a number between 0.0 and \n",
    "        1.0)\n",
    "    '''\n",
    "    # code is correct, but gives a warning about the model not having a predefined maximum length,\n",
    "    # suppressing those warnings to not interfere with tdqm progress bar\n",
    "    import warnings\n",
    "    from transformers import logging\n",
    "    warnings.filterwarnings('ignore')\n",
    "    warnings.simplefilter('ignore')\n",
    "    logging.set_verbosity_error()\n",
    "    \n",
    "    # read in gutenberg book\n",
    "    book_text =  get_book_text(gutenberg_id)\n",
    "\n",
    "    # load NER model and tokenizer\n",
    "    ner_model = 'Jean-Baptiste/camembert-ner'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ner_model)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(ner_model)\n",
    "    nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    # prepare for iteration over the book\n",
    "    sentence_level_book = re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', book_text)\n",
    "    ner_entities_tokens = []\n",
    "    ner_entities_words = []\n",
    "    sentence_index = 0\n",
    "    \n",
    "    # iterate over sentence-level chunks\n",
    "    for l in tqdm(sentence_level_book):\n",
    "        if len(l) > max_chunk_len:\n",
    "            for m in range(len(l) // split_chunk_len + 1):\n",
    "                new_l = ' '.join(l.split(' ')[m*split_chunk_len:][:(m+1)*split_chunk_len])\n",
    "                ner_entities_tokens, ner_entities_words, sentence_index = get_line_entities(new_l, \n",
    "                                                                                            ner_entities_tokens, \n",
    "                                                                                            ner_entities_words,\n",
    "                                                                                            sentence_index, \n",
    "                                                                                            tokenizer,\n",
    "                                                                                            nlp,\n",
    "                                                                                            grouped_entities)\n",
    "        else:\n",
    "            ner_entities_tokens, ner_entities_words, sentence_index = get_line_entities(l, \n",
    "                                                                                        ner_entities_tokens, \n",
    "                                                                                        ner_entities_words,\n",
    "                                                                                        sentence_index, \n",
    "                                                                                        tokenizer, \n",
    "                                                                                        nlp,\n",
    "                                                                                        grouped_entities)\n",
    "\n",
    "    return book_text, ner_entities_tokens, ner_entities_words"
   ]
  },
  {
   "cell_type": "raw",
   "id": "919159a6-db8f-4ff2-b453-a17c9a3592f8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "split_chunk_len = 256\n",
    "max_chunk_len=512\n",
    "\n",
    "# iterate over sentence-level chunks\n",
    "for l in tqdm(sentence_level_book):\n",
    "    \n",
    "    if len(l) > max_chunk_len:\n",
    "        for m in range(len(l) // split_chunk_len + 1):\n",
    "            new_l = ' '.join(l.split(' ')[m*split_chunk_len:][:(m+1)*split_chunk_len])\n",
    "            print(new_l)\n",
    "            \"\"\"ner_entities_tokens, ner_entities_words, sentence_index = get_line_entities(new_l, \n",
    "                                                                                        ner_entities_tokens, \n",
    "                                                                                        ner_entities_words,\n",
    "                                                                                        sentence_index, \n",
    "                                                                                        tokenizer,\n",
    "                                                                                        nlp,\n",
    "                                                                                        grouped_entities)\"\"\"\n",
    "    else:\n",
    "        \"\"\"ner_entities_tokens, ner_entities_words, sentence_index = get_line_entities(l, \n",
    "                                                                                    ner_entities_tokens, \n",
    "                                                                                    ner_entities_words,\n",
    "                                                                                    sentence_index, \n",
    "                                                                                    tokenizer, \n",
    "                                                                                    nlp,\n",
    "                                                                                    grouped_entities)\"\"\"\n",
    "                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "127d49f4-7394-49f5-bd32-d1b5c2c847fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7929/7929 [25:43<00:00,  5.14it/s]  \n"
     ]
    }
   ],
   "source": [
    "rouge_noir_id = '798-8'\n",
    "(rouge_noir_text, \n",
    " rouge_noir_ent_tokens, \n",
    " rouge_noir_ent_words) = get_person_entities(rouge_noir_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a309897b-4d20-4394-b03c-5c412948110c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_word\n",
       "julien      577\n",
       "é           377\n",
       "mme         233\n",
       "m           178\n",
       "r           160\n",
       "la          105\n",
       "mathilde     97\n",
       "è            80\n",
       "valenod      73\n",
       "mlle         66\n",
       "mole         55\n",
       "ê            50\n",
       "le           38\n",
       "marquis      38\n",
       "ch           37\n",
       "l            36\n",
       "dit          33\n",
       "tait         32\n",
       "e            30\n",
       "sorel        26\n",
       "fouqu        25\n",
       "re           24\n",
       "comme        21\n",
       "disait       21\n",
       "si           21\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save entities into a dataframe, and to the disk\n",
    "rouge_noir_df = pd.DataFrame(rouge_noir_ent_words)\n",
    "rouge_noir_df['full_word'] =  rouge_noir_df['full_word'].apply(lambda s: s.lower())\n",
    "rouge_noir_df.to_csv('../data/book_dfs/rouge_noir_df.csv', index=False) \n",
    "\n",
    "# view top 25 entities\n",
    "(rouge_noir_df\n",
    " .drop_duplicates('total_word_index')\n",
    " .groupby('full_word')\n",
    " .count()\n",
    " .sort_values(by='score', ascending=False)\n",
    ")['score'][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2563a6-7f48-4403-a61d-142df93671cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
