{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2057c890-73f3-4c01-9762-5dba08d2221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/eloisedoyard/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/eloisedoyard/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/eloisedoyard/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/eloisedoyard/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/eloisedoyard/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from transformers import BertConfig, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, \\\n",
    "                         AutoTokenizer, AutoModelForTokenClassification, pipeline, BertTokenizer, BertModel, \\\n",
    "                         LukeTokenizer, LukeForEntitySpanClassification\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ba66e2-6043-463f-acb8-3f59d30ce3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_text(gutenberg_id):\n",
    "    '''Given a book ID, returns the book's text, excluding Project Gutenberg's header and outro.\n",
    "    \n",
    "     Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    book_text : str\n",
    "        The book's text, excluding Project Gutenberg's header and outro\n",
    "    '''\n",
    "    \n",
    "    context = ''\n",
    "    with open(f'../data/book/PG-{gutenberg_id}.txt', mode='r', encoding='utf-8') as f:\n",
    "        context = f.read()\n",
    "    return ' '.join([l for l in (context.split('End of the Project Gutenberg EBook of ')[0]\n",
    "                                        .split('*** END OF THE PROJECT GUTENBERG EBOOK')[0]\n",
    "                                        .split('\\n')) if l][16:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d91f53a-fc03-400f-a514-4ce6d59359d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertModel, CamembertTokenizer\n",
    "from transformers import FlaubertTokenizer, FlaubertModel\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6477ccb4-09a1-46e9-b628-e6d4f4434155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_entities(l, ner_entities_tokens, ner_entities_words, sentence_index, tokenizer, nlp,\n",
    "                     grouped_entities):\n",
    "    '''Given a line, lists for tokens and words, and word index at the end of the sentence, as well as\n",
    "    the tokenizer and nlp model instances (from huggingface's transformers), updates the tokens and\n",
    "    words lists and the word index to include the given line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    l : str\n",
    "        The line to analyze\n",
    "    ner_entities_tokens : list\n",
    "        A list containing all the Person tokens found so far, across all the previous lines\n",
    "    ner_entities_words : list\n",
    "        A list containing dictionary entries of all the Person entities found so far (the full \n",
    "        word corresponding to them (i.e. not separated tokens), their index in the sentence and \n",
    "        in the book overall, and their PER-entity classification score, a number between 0.0 and \n",
    "        1.0), across all the previous lines\n",
    "    sentence_index : int\n",
    "        The overall (book-wise) index of the first word of the sentence\n",
    "    tokenizer : AutoTokenizer\n",
    "        huggingface's tokenizer being used in the NER pipeline\n",
    "    nlp : pipeline\n",
    "        huggingface's NER pipeline object\n",
    "    grouped_entities : bool\n",
    "        Flag indicating whether the NER pipeline is configured to output grouped_entities or not\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ner_entities_tokens : list\n",
    "        A list containing all the Person tokens found so far, across this and all the previous lines\n",
    "    ner_entities_words : list\n",
    "        A list containing dictionary entries of all the Person entities found so far (the full \n",
    "        word corresponding to them (i.e. not separated tokens), their index in the sentence and \n",
    "        in the book overall, and their PER-entity classification score, a number between 0.0 and \n",
    "        1.0), across this and all the previous lines\n",
    "    sentence_index : int\n",
    "        The overall (book-wise) index of the first word of the next sentence\n",
    "    '''\n",
    "    new_entity_tokens = []\n",
    "    if grouped_entities:\n",
    "        new_entity_tokens = [e for e in nlp(l) if 'PER' in e['entity_group']]\n",
    "    else:\n",
    "        new_entity_tokens = [e for e in nlp(l) if 'PER' in e['entity']]\n",
    "    ner_entities_tokens += new_entity_tokens\n",
    "    \n",
    "    tokenized_line = tokenizer(l)\n",
    "    line_words = [w if w != 'word_tokenize_splits_cannot_into_2_words' else 'cannot'\n",
    "                    for w in word_tokenize(\n",
    "                                           re.sub(r'[^a-zA-Z0-9]', ' \\g<0> ', \n",
    "                                                  l).replace('cannot', \n",
    "                                                            'word_tokenize_splits_cannot_into_2_words'))]\n",
    "    \n",
    "    # go from token to word with\n",
    "    for et in new_entity_tokens:\n",
    "        if grouped_entities:\n",
    "            # find index of grouped entity\n",
    "            reconstructed_line = ' '.join([lw.lower() for lw in line_words])\n",
    "            first_word = word_tokenize(re.sub(r'[^a-zA-Z0-9]', ' \\g<0> ', et['word']))[0]\n",
    "            if et['word'][0] == '#':\n",
    "                first_word = word_tokenize(re.sub(r'[^a-zA-Z0-9]', ' \\g<0> ', et['word'][2:]))[0]\n",
    "            \n",
    "            word_index = len(reconstructed_line[:reconstructed_line.index(first_word)].split())\n",
    "\n",
    "            if et['word'] not in stopwords.words('french') and et['word'].isalpha():\n",
    "                # record grouped entity\n",
    "                ner_entities_words += [{'full_word': et['word'], \n",
    "                                        'sentence_word_index': word_index, \n",
    "                                        'total_word_index': sentence_index+word_index,\n",
    "                                        'score': et['score']}]\n",
    "        else:\n",
    "            # record non-grouped entity\n",
    "            word_index = tokenized_line.word_ids()[et['index']]\n",
    "            if line_words[word_index] not in stopwords.words('french') and line_words[word_index].isalpha():\n",
    "                ner_entities_words += [{'full_word': line_words[word_index], \n",
    "                                        'sentence_word_index': word_index, \n",
    "                                        'total_word_index': sentence_index+word_index,\n",
    "                                        'score': et['score']}]\n",
    "    sentence_index += len(line_words)\n",
    "    return ner_entities_tokens, ner_entities_words, sentence_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e80c1758-bfeb-4d00-8fac-30913c30adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_person_entities(gutenberg_id, grouped_entities=False, max_chunk_len=512, split_chunk_len=256):\n",
    "    '''Given a book ID, returns its text (excluding Project Gutenberg's intro and outro), all its \n",
    "    tokens classified as PER (Person) entities, and all the words corresponding to those tokens, as \n",
    "    well as their index in the sentence and in the book, and their classification score as a PER entity.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    grouped_entities : bool, optional\n",
    "        Flag indicating whether the NER pipeline is configured to outout grouped_entities or not \n",
    "        (default is False)\n",
    "    max_chunk_len : int, optional\n",
    "        Maximum character-level length of each sentence passed to the model (default is 512)\n",
    "    split_chunk_len : int, optional\n",
    "        Maximum character-level length of each sub-sentence passed to the model, when splitting an\n",
    "        overly big sentence into smaller sub-sentences (default is 256)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    book_text : str\n",
    "        The book's text, excluding Project Gutenberg's header and outro\n",
    "    ner_entities_tokens : list\n",
    "        A list containing all the Person tokens found across the whole book\n",
    "    ner_entities_words : list\n",
    "        A list containing dictionary entries of all the Person entities found across the whole book \n",
    "        (the full word corresponding to them (i.e. not separated tokens), their index in the sentence \n",
    "        and in the book overall, and their PER-entity classification score, a number between 0.0 and \n",
    "        1.0)\n",
    "    '''\n",
    "    # code is correct, but gives a warning about the model not having a predefined maximum length,\n",
    "    # suppressing those warnings to not interfere with tdqm progress bar\n",
    "    import warnings\n",
    "    from transformers import logging\n",
    "    warnings.filterwarnings('ignore')\n",
    "    warnings.simplefilter('ignore')\n",
    "    logging.set_verbosity_error()\n",
    "    \n",
    "    # read in gutenberg book\n",
    "    book_text =  get_book_text(gutenberg_id)\n",
    "\n",
    "    # load NER model and tokenizer\n",
    "    ner_model = 'camembert'\n",
    "    tokenizer = FlaubertTokenizer.from_pretrained(\"flaubert/flaubert_base_cased\")\n",
    "    model = FlaubertModel.from_pretrained(\"flaubert/flaubert_base_cased\")\n",
    "    nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    # prepare for iteration over the book\n",
    "    sentence_level_book = re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', book_text)\n",
    "    ner_entities_tokens = []\n",
    "    ner_entities_words = []\n",
    "    sentence_index = 0\n",
    "    \n",
    "    # iterate over sentence-level chunks\n",
    "    for l in tqdm(sentence_level_book):\n",
    "        if len(l) > max_chunk_len:\n",
    "            for m in range(len(l) // split_chunk_len + 1):\n",
    "                new_l = ' '.join(l.split(' ')[m*split_chunk_len:][:(m+1)*split_chunk_len])\n",
    "                ner_entities_tokens, ner_entities_words, sentence_index = get_line_entities(new_l, \n",
    "                                                                                            ner_entities_tokens, \n",
    "                                                                                            ner_entities_words,\n",
    "                                                                                            sentence_index, \n",
    "                                                                                            tokenizer,\n",
    "                                                                                            nlp,\n",
    "                                                                                            grouped_entities)\n",
    "        else:\n",
    "            ner_entities_tokens, ner_entities_words, sentence_index = get_line_entities(l, \n",
    "                                                                                        ner_entities_tokens, \n",
    "                                                                                        ner_entities_words,\n",
    "                                                                                        sentence_index, \n",
    "                                                                                        tokenizer, \n",
    "                                                                                        nlp,\n",
    "                                                                                        grouped_entities)\n",
    "\n",
    "    return book_text, ner_entities_tokens, ner_entities_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "127d49f4-7394-49f5-bd32-d1b5c2c847fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81754fa065184631939b1ae437dd23cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.56M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fa78aa179b4d6fb1544d2999a1fe10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/896k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f417cc83c854be1b8b1ddffa4716b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7c68d42f1b4f0f9e7d7a992f5cdf8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5baea51cc14c28902dee5e4fe3309b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/553M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "PipelineException",
     "evalue": "The model 'FlaubertModel' is not supported for ner. Supported models are ['BigBirdForTokenClassification', 'ConvBertForTokenClassification', 'LayoutLMForTokenClassification', 'DistilBertForTokenClassification', 'CamembertForTokenClassification', 'FlaubertForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'LongformerForTokenClassification', 'RobertaForTokenClassification', 'SqueezeBertForTokenClassification', 'BertForTokenClassification', 'MegatronBertForTokenClassification', 'MobileBertForTokenClassification', 'XLNetForTokenClassification', 'AlbertForTokenClassification', 'ElectraForTokenClassification', 'FunnelForTokenClassification', 'MPNetForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'IBertForTokenClassification']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPipelineException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1ebf8213f243>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m (rouge_noir_text, \n\u001b[1;32m      3\u001b[0m  \u001b[0mrouge_noir_ent_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m  rouge_noir_ent_words) = get_person_entities(rouge_noir_id)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-b7a2f22872b5>\u001b[0m in \u001b[0;36mget_person_entities\u001b[0;34m(gutenberg_id, grouped_entities, max_chunk_len, split_chunk_len)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlaubertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"flaubert/flaubert_base_cased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlaubertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"flaubert/flaubert_base_cased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ner\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# prepare for iteration over the book\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, framework, revision, use_fast, use_auth_token, model_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtask_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelcard\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodelcard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/pipelines/token_classification.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, modelcard, framework, args_parser, device, binary_output, ignore_labels, task, grouped_entities, ignore_subwords)\u001b[0m\n\u001b[1;32m     98\u001b[0m         )\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         self.check_model_type(\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0mTF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mcheck_model_type\u001b[0;34m(self, supported_models)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0msupported_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msupported_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msupported_models\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             raise PipelineException(\n\u001b[0m\u001b[1;32m    636\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPipelineException\u001b[0m: The model 'FlaubertModel' is not supported for ner. Supported models are ['BigBirdForTokenClassification', 'ConvBertForTokenClassification', 'LayoutLMForTokenClassification', 'DistilBertForTokenClassification', 'CamembertForTokenClassification', 'FlaubertForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'LongformerForTokenClassification', 'RobertaForTokenClassification', 'SqueezeBertForTokenClassification', 'BertForTokenClassification', 'MegatronBertForTokenClassification', 'MobileBertForTokenClassification', 'XLNetForTokenClassification', 'AlbertForTokenClassification', 'ElectraForTokenClassification', 'FunnelForTokenClassification', 'MPNetForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'IBertForTokenClassification']"
     ]
    }
   ],
   "source": [
    "rouge_noir_id = '798-8'\n",
    "(rouge_noir_text, \n",
    " rouge_noir_ent_tokens, \n",
    " rouge_noir_ent_words) = get_person_entities(rouge_noir_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3017c915-8367-4acf-a8a0-e3a9cdc40fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8c7a3057264cc9be6ab0411b9259eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/508 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f2a4d5e8254d0f9493f181a408be16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/811k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8b28079a06460c9d4dbf5db935162c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18610ee19a35489abd54448c847c8d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7929/7929 [25:35<00:00,  5.16it/s]  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'full_word'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'full_word'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-64c36b936047>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# save entities into a dataframe, and to the disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrouge_noir_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouge_noir_ent_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrouge_noir_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_word'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mrouge_noir_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mrouge_noir_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/book_dfs/rouge_noir_df.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'full_word'"
     ]
    }
   ],
   "source": [
    "# save entities into a dataframe, and to the disk\n",
    "rouge_noir_df = pd.DataFrame(rouge_noir_ent_words)\n",
    "rouge_noir_df['full_word'] =  rouge_noir_df['full_word'].apply(lambda s: s.lower())\n",
    "rouge_noir_df.to_csv('../data/book_dfs/rouge_noir_df.csv', index=False) \n",
    "\n",
    "# view top 25 entities\n",
    "(rouge_noir_df\n",
    " .drop_duplicates('total_word_index')\n",
    " .groupby('full_word')\n",
    " .count()\n",
    " .sort_values(by='score', ascending=False)\n",
    ")['score'][:25]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
