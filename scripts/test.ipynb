{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-LOC',\n",
       "  'score': 0.55669767,\n",
       "  'index': 10,\n",
       "  'word': '▁d',\n",
       "  'start': 36,\n",
       "  'end': 38},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.95578015,\n",
       "  'index': 12,\n",
       "  'word': 'Italie',\n",
       "  'start': 39,\n",
       "  'end': 45},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9981791,\n",
       "  'index': 16,\n",
       "  'word': '▁V',\n",
       "  'start': 55,\n",
       "  'end': 57},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9979278,\n",
       "  'index': 17,\n",
       "  'word': 'errière',\n",
       "  'start': 57,\n",
       "  'end': 64},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99829894,\n",
       "  'index': 18,\n",
       "  'word': 's',\n",
       "  'start': 64,\n",
       "  'end': 65},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9855219,\n",
       "  'index': 31,\n",
       "  'word': '▁M',\n",
       "  'start': 112,\n",
       "  'end': 114},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9845025,\n",
       "  'index': 32,\n",
       "  'word': '.',\n",
       "  'start': 114,\n",
       "  'end': 115},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.8523049,\n",
       "  'index': 33,\n",
       "  'word': '▁le',\n",
       "  'start': 115,\n",
       "  'end': 118},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.87298524,\n",
       "  'index': 34,\n",
       "  'word': '▁maire',\n",
       "  'start': 118,\n",
       "  'end': 124},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.92345726,\n",
       "  'index': 74,\n",
       "  'word': '▁M',\n",
       "  'start': 261,\n",
       "  'end': 263},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.91418386,\n",
       "  'index': 75,\n",
       "  'word': '.',\n",
       "  'start': 263,\n",
       "  'end': 264},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9488357,\n",
       "  'index': 76,\n",
       "  'word': '▁de',\n",
       "  'start': 264,\n",
       "  'end': 267},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99203897,\n",
       "  'index': 77,\n",
       "  'word': '▁R',\n",
       "  'start': 267,\n",
       "  'end': 269},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99149936,\n",
       "  'index': 78,\n",
       "  'word': 'ê',\n",
       "  'start': 269,\n",
       "  'end': 270},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9911446,\n",
       "  'index': 79,\n",
       "  'word': 'nal',\n",
       "  'start': 270,\n",
       "  'end': 273},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9943797,\n",
       "  'index': 96,\n",
       "  'word': '▁Légion',\n",
       "  'start': 355,\n",
       "  'end': 362},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9943036,\n",
       "  'index': 97,\n",
       "  'word': '▁d',\n",
       "  'start': 362,\n",
       "  'end': 364},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9942684,\n",
       "  'index': 99,\n",
       "  'word': 'honneur',\n",
       "  'start': 365,\n",
       "  'end': 372}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading https://files.pythonhosted.org/packages/a4/54/81b1c3c574a1ffde54b0c82ed2a37d81395709cdd5f50e59970aeed5d95e/torch-1.10.2-cp36-cp36m-manylinux1_x86_64.whl (881.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 881.9MB 1.8kB/s eta 0:00:01  3% |█▏                              | 31.2MB 77.8MB/s eta 0:00:11    16% |█████▍                          | 147.3MB 80.5MB/s eta 0:00:10    19% |██████▏                         | 170.9MB 72.3MB/s eta 0:00:10    28% |█████████                       | 248.8MB 78.0MB/s eta 0:00:09    40% |█████████████                   | 355.3MB 74.2MB/s eta 0:00:08    44% |██████████████▏                 | 391.2MB 75.7MB/s eta 0:00:070.5MB 75.2MB/s eta 0:00:06��████████████████▎             | 504.5MB 74.1MB/s eta 0:00:06�██▌             | 508.5MB 80.4MB/s eta 0:00:057MB 74.1MB/s eta 0:00:05�███████████▉             | 518.4MB 70.3MB/s eta 0:00:06█             | 522.7MB 81.3MB/s eta 0:00:05█████████████             | 526.5MB 79.8MB/s eta 0:00:05��███████████▎            | 531.0MB 68.1MB/s eta 0:00:06�███▍            | 534.8MB 73.5MB/s eta 0:00:05██████████▌            | 538.3MB 76.1MB/s eta 0:00:05��█▊            | 543.5MB 71.7MB/s eta 0:00:05�████████▉            | 547.1MB 76.8MB/s eta 0:00:05█            | 550.8MB 72.8MB/s eta 0:00:05 |████████████████████▎           | 557.9MB 73.3MB/s eta 0:00:05�███████████████████▍           | 562.0MB 77.2MB/s eta 0:00:05ta 0:00:05█████████████████▋           | 568.4MB 78.3MB/s eta 0:00:05:04 |████████████████████▉           | 575.3MB 70.9MB/s eta 0:00:05/s eta 0:00:04��█████████████▎          | 585.1MB 82.2MB/s eta 0:00:04.2MB/s eta 0:00:04��██████████▌          | 592.2MB 76.2MB/s eta 0:00:04|█████████████████████▋          | 595.4MB 79.7MB/s eta 0:00:04��████▉          | 601.8MB 78.4MB/s eta 0:00:04�████████          | 607.5MB 68.1MB/s eta 0:00:05�█████████████████████▏         | 611.3MB 74.5MB/s eta 0:00:04�▎         | 614.3MB 72.7MB/s eta 0:00:04 0:00:044�███████████████▊         | 627.3MB 80.5MB/s eta 0:00:04�███████         | 631.5MB 72.3MB/s eta 0:00:0436.1MB 75.0MB/s eta 0:00:04% |███████████████████████▏        | 640.0MB 70.5MB/s eta 0:00:04��██████▎        | 642.8MB 72.1MB/s eta 0:00:04��████████████████▌        | 646.3MB 75.1MB/s eta 0:00:04�        | 649.1MB 75.2MB/s eta 0:00:04�████▋        | 651.9MB 71.8MB/s eta 0:00:04030359.9MB 58.6MB/s eta 0:00:040:00:03��███▏       | 665.3MB 71.6MB/s eta 0:00:04 75% |████████████████████████▎       | 668.4MB 53.6MB/s eta 0:00:04 76% |████████████████████████▍       | 671.0MB 72.6MB/s eta 0:00:03    76% |████████████████████████▍       | 673.6MB 74.3MB/s eta 0:00:037% |████████████████████████▊       | 680.5MB 66.1MB/s eta 0:00:047% |████████████████████████▉       | 682.9MB 69.0MB/s eta 0:00:03��████████▏      | 693.1MB 72.0MB/s eta 0:00:03�      | 695.5MB 79.1MB/s eta 0:00:03MB 52.3MB/s eta 0:00:04 701.5MB 69.0MB/s eta 0:00:033██████▋      | 706.4MB 75.3MB/s eta 0:00:03 | 710.2MB 80.1MB/s eta 0:00:03�█████████████████████      | 715.4MB 76.7MB/s eta 0:00:03�███████████████████████      | 719.3MB 70.9MB/s eta 0:00:03████▏     | 721.7MB 53.8MB/s eta 0:00:03███████████████████████▎     | 724.1MB 78.2MB/s eta 0:00:03��███████████████████████▍     | 726.6MB 76.4MB/s eta 0:00:03��██████▌     | 729.4MB 71.1MB/s eta 0:00:03███▋     | 732.2MB 74.6MB/s eta 0:00:03�     | 735.7MB 79.0MB/s eta 0:00:02�████████▉     | 738.6MB 71.5MB/s eta 0:00:03██████████████████████████     | 744.9MB 76.7MB/s eta 0:00:02MB 77.7MB/s eta 0:00:02   85% |███████████████████████████▎    | 751.9MB 81.7MB/s eta 0:00:02ta 0:00:03███████████▌    | 757.4MB 74.6MB/s eta 0:00:02█████▋    | 759.6MB 73.9MB/s eta 0:00:028MB 72.8MB/s eta 0:00:02�███████████████████    | 770.5MB 74.1MB/s eta 0:00:02██████████    | 774.7MB 82.1MB/s eta 0:00:02██████████████▎   | 778.3MB 75.0MB/s eta 0:00:02% |████████████████████████████▍   | 781.0MB 74.4MB/s eta 0:00:02   88% |████████████████████████████▍   | 783.5MB 57.8MB/s eta 0:00:02�██▌   | 786.1MB 71.4MB/s eta 0:00:02�███████████████████▋   | 787.8MB 72.7MB/s eta 0:00:02   89% |████████████████████████████▊   | 790.4MB 57.2MB/s eta 0:00:02�██▊   | 793.0MB 74.6MB/s eta 0:00:02 0:00:02███████████████▏  | 803.9MB 69.4MB/s eta 0:00:02��███▎  | 806.5MB 78.3MB/s eta 0:00:01.9MB 56.6MB/s eta 0:00:02�███▍  | 811.4MB 71.8MB/s eta 0:00:01▌  | 814.4MB 78.8MB/s eta 0:00:01 78.3MB/s eta 0:00:01��████████████████▉  | 822.6MB 75.9MB/s eta 0:00:01��██████████████████████  | 825.1MB 79.4MB/s eta 0:00:01██████  | 828.9MB 70.9MB/s eta 0:00:01███████▏ | 831.5MB 72.3MB/s eta 0:00:01��██████████████████████████▎ | 834.6MB 81.2MB/s eta 0:00:01████████▌ | 840.6MB 81.1MB/s eta 0:00:01 | 843.9MB 78.7MB/s eta 0:00:01████████████████████████████▊ | 847.5MB 73.3MB/s eta 0:00:0116% |███████████████████████████████ | 852.4MB 61.0MB/s eta 0:00:01�| 858.2MB 80.1MB/s eta 0:00:01�████████████████████████▏| 860.3MB 58.0MB/s eta 0:00:017% |███████████████████████████████▎| 862.5MB 78.6MB/s eta 0:00:01��███████████▍| 864.7MB 80.4MB/s eta 0:00:01��███████████▌| 867.4MB 77.6MB/s eta 0:00:01�███████████████▋| 870.3MB 55.2MB/s eta 0:00:01�██████████████████▊| 873.7MB 77.0MB/s eta 0:00:01��████▉| 876.9MB 70.4MB/s eta 0:00:01�██████████████████| 879.3MB 57.0MB/s eta 0:00:01�██████████████████████████| 881.5MB 77.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses; python_version < \"3.7\" (from torch)\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl\n",
      "Collecting typing-extensions (from torch)\n",
      "  Using cached https://files.pythonhosted.org/packages/45/6b/44f7f8f1e110027cf88956b59f2fad776cca7e1704396d043f89effd3a0e/typing_extensions-4.1.1-py3-none-any.whl\n",
      "Installing collected packages: dataclasses, typing-extensions, torch\n",
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pip/commands/install.py\", line 342, in run\n",
      "    prefix=options.prefix_path,\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pip/req/req_set.py\", line 784, in install\n",
      "    **kwargs\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pip/req/req_install.py\", line 851, in install\n",
      "    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pip/req/req_install.py\", line 1064, in move_wheel_files\n",
      "    isolated=self.isolated,\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pip/wheel.py\", line 345, in move_wheel_files\n",
      "    clobber(source, lib_dir, True)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pip/wheel.py\", line 323, in clobber\n",
      "    shutil.copyfile(srcfile, destfile)\n",
      "  File \"/opt/anaconda3/lib/python3.6/shutil.py\", line 121, in copyfile\n",
      "    with open(dst, 'wb') as fdst:\n",
      "PermissionError: [Errno 13] Permission denied: '/opt/anaconda3/lib/python3.6/site-packages/dataclasses.py'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading https://files.pythonhosted.org/packages/38/03/c963ecdf98fae15286437ae533750e2c39e988b7d8c86fad4dbc73a3a146/torchvision-0.11.2-cp36-cp36m-manylinux1_x86_64.whl (23.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 23.3MB 69kB/s  eta 0:00:01   17% |█████▌                          | 4.0MB 58.9MB/s eta 0:00:01    44% |██████████████▍                 | 10.4MB 61.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/anaconda3/lib/python3.6/site-packages (from torchvision)\n",
      "Collecting pillow!=8.3.0,>=5.3.0 (from torchvision)\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/2a/2fc11b54e2742db06297f7fa7f420a0e3069fdcf0e4b57dfec33f0b08622/Pillow-8.4.0.tar.gz (49.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 49.4MB 28kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting torch==1.10.1 (from torchvision)\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/f5/b76d021f06e50f770d3f6c1a1b50b62a69e587b1f0db7248269c4be21206/torch-1.10.1-cp36-cp36m-manylinux1_x86_64.whl (881.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 881.9MB 1.8kB/s eta 0:00:01    13% |████▎                           | 116.9MB 71.4MB/s eta 0:00:11    25% |████████                        | 222.5MB 75.4MB/s eta 0:00:09    29% |█████████▎                      | 257.2MB 74.5MB/s eta 0:00:09    30% |██████████                      | 272.6MB 72.6MB/s eta 0:00:09    41% |█████████████▍                  | 368.2MB 77.1MB/s eta 0:00:07    61% |███████████████████▊            | 544.7MB 71.5MB/s eta 0:00:05    64% |████████████████████▊           | 570.7MB 76.9MB/s eta 0:00:05    68% |██████████████████████          | 606.1MB 76.1MB/s eta 0:00:04    74% |███████████████████████▊        | 654.0MB 73.3MB/s eta 0:00:04    76% |████████████████████████▋       | 677.8MB 72.7MB/s eta 0:00:03    79% |█████████████████████████▎      | 697.8MB 75.1MB/s eta 0:00:03    85% |███████████████████████████▍    | 753.6MB 75.4MB/s eta 0:00:02    88% |████████████████████████████▏   | 777.2MB 76.3MB/s eta 0:00:02    89% |████████████████████████████▉   | 793.2MB 72.1MB/s eta 0:00:02   93% |█████████████████████████████▉  | 823.3MB 71.0MB/s eta 0:00:01█████████████████████  | 825.6MB 73.4MB/s eta 0:00:01█████████████▏ | 831.1MB 74.6MB/s eta 0:00:0101�█████████████▎ | 835.7MB 68.2MB/s eta 0:00:01B 72.6MB/s eta 0:00:01��███████▌ | 839.8MB 73.1MB/s eta 0:00:01��█████████████████████████▋ | 842.4MB 70.8MB/s eta 0:00:01�████████▋ | 844.5MB 72.5MB/s eta 0:00:01��████████████████▊ | 846.6MB 69.1MB/s eta 0:00:01�█████████████▉ | 848.9MB 69.2MB/s eta 0:00:01B 67.3MB/s eta 0:00:0101████████ | 855.5MB 66.1MB/s eta 0:00:01��███████████▏| 859.5MB 68.2MB/s eta 0:00:01 97% |███████████████████████████████▎| 862.5MB 67.0MB/s eta 0:00:01�██████████████▌| 868.0MB 72.1MB/s eta 0:00:01/s eta 0:00:01███████████████▉| 878.4MB 71.5MB/s eta 0:00:01�██████████████████| 880.2MB 44.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses; python_version < \"3.7\" (from torch==1.10.1->torchvision)\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl\n",
      "Collecting typing-extensions (from torch==1.10.1->torchvision)\n",
      "  Using cached https://files.pythonhosted.org/packages/45/6b/44f7f8f1e110027cf88956b59f2fad776cca7e1704396d043f89effd3a0e/typing_extensions-4.1.1-py3-none-any.whl\n",
      "Building wheels for collected packages: pillow\n",
      "\u001b[33m  Building wheel for pillow failed: [Errno 28] No space left on device: '/home/doyard/.cache/pip/wheels'\u001b[0m\n",
      "Failed to build pillow\n",
      "Installing collected packages: pillow, dataclasses, typing-extensions, torch, torchvision\n",
      "  Found existing installation: Pillow 4.2.1\n",
      "    Uninstalling Pillow-4.2.1:\n",
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.6/shutil.py\", line 544, in move\n",
      "    os.rename(src, real_dst)\n",
      "OSError: [Errno 18] Invalid cross-device link: '/opt/anaconda3/lib/python3.6/site-packages/PIL' -> '/scratch/students/doyard/sentiment-evolution-in-characters-network/scripts/pip-hkedrkr0-uninstall/opt/anaconda3/lib/python3.6/site-packages/PIL'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pip/commands/install.py\", line 342, in run\n",
      "    prefix=options.prefix_path,\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pip/req/req_set.py\", line 778, in install\n",
      "    requirement.uninstall(auto_confirm=True)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pip/req/req_install.py\", line 754, in uninstall\n",
      "    paths_to_remove.remove(auto_confirm)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pip/req/req_uninstall.py\", line 115, in remove\n",
      "    renames(path, new_path)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pip/utils/__init__.py\", line 267, in renames\n",
      "    shutil.move(old, new)\n",
      "  File \"/opt/anaconda3/lib/python3.6/shutil.py\", line 556, in move\n",
      "    rmtree(src)\n",
      "  File \"/opt/anaconda3/lib/python3.6/shutil.py\", line 480, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/opt/anaconda3/lib/python3.6/shutil.py\", line 438, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/opt/anaconda3/lib/python3.6/shutil.py\", line 436, in _rmtree_safe_fd\n",
      "    os.unlink(name, dir_fd=topfd)\n",
      "PermissionError: [Errno 13] Permission denied: 'SgiImagePlugin.py'\u001b[0m\n",
      "Collecting torchgeometry\n",
      "  Downloading https://files.pythonhosted.org/packages/a6/d6/3f6820c0589bc3876080c59b58a3bad11af746a7b46f364b1cde7972bd72/torchgeometry-0.1.2-py2.py3-none-any.whl (42kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 5.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch>=1.0.0 (from torchgeometry)\n",
      "  Downloading https://files.pythonhosted.org/packages/a4/54/81b1c3c574a1ffde54b0c82ed2a37d81395709cdd5f50e59970aeed5d95e/torch-1.10.2-cp36-cp36m-manylinux1_x86_64.whl (881.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 881.9MB 1.8kB/s eta 0:00:01 0% |▎                               | 8.5MB 68.2MB/s eta 0:00:13    9% |███                             | 85.2MB 67.2MB/s eta 0:00:12    15% |█████                           | 139.7MB 68.5MB/s eta 0:00:11    21% |███████                         | 190.7MB 68.8MB/s eta 0:00:11    52% |█████████████████               | 465.8MB 75.2MB/s eta 0:00:06    61% |███████████████████▊            | 543.2MB 73.8MB/s eta 0:00:05    78% |█████████████████████████▎      | 696.3MB 72.1MB/s eta 0:00:03    85% |███████████████████████████▍    | 755.4MB 73.8MB/s eta 0:00:02    88% |████████████████████████████▎   | 779.1MB 73.6MB/s eta 0:00:02    92% |█████████████████████████████▊  | 818.3MB 72.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses; python_version < \"3.7\" (from torch>=1.0.0->torchgeometry)\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl\n",
      "Collecting typing-extensions (from torch>=1.0.0->torchgeometry)\n",
      "  Using cached https://files.pythonhosted.org/packages/45/6b/44f7f8f1e110027cf88956b59f2fad776cca7e1704396d043f89effd3a0e/typing_extensions-4.1.1-py3-none-any.whl\n",
      "Installing collected packages: dataclasses, typing-extensions, torch, torchgeometry\n",
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pip/commands/install.py\", line 342, in run\n",
      "    prefix=options.prefix_path,\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pip/req/req_set.py\", line 784, in install\n",
      "    **kwargs\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pip/req/req_install.py\", line 851, in install\n",
      "    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pip/req/req_install.py\", line 1064, in move_wheel_files\n",
      "    isolated=self.isolated,\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pip/wheel.py\", line 345, in move_wheel_files\n",
      "    clobber(source, lib_dir, True)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pip/wheel.py\", line 323, in clobber\n",
      "    shutil.copyfile(srcfile, destfile)\n",
      "  File \"/opt/anaconda3/lib/python3.6/shutil.py\", line 121, in copyfile\n",
      "    with open(dst, 'wb') as fdst:\n",
      "PermissionError: [Errno 13] Permission denied: '/opt/anaconda3/lib/python3.6/site-packages/dataclasses.py'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching package metadata ...........\n",
      "Solving package specifications: .\n",
      "\n",
      "Package plan for installation in environment /opt/anaconda3:\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    _libgcc_mutex:          0.1-main             \n",
      "    conda-package-handling: 1.7.3-py36h27cfd23_1 \n",
      "    tqdm:                   4.63.0-pyhd3eb1b0_0  \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    conda:                  4.3.30-py36h5d9f9f4_0 --> 4.10.3-py36h06a4308_0\n",
      "    conda-env:              2.6.0-h36134e3_1      --> 2.6.0-1              \n",
      "    libgcc-ng:              7.2.0-h7cc24e2_2      --> 9.1.0-hdf63c60_0     \n",
      "    pycosat:                0.6.2-py36h1a0ea17_1  --> 0.6.3-py36h27cfd23_0 \n",
      "\n",
      "Proceed ([y]/n)? "
     ]
    }
   ],
   "source": [
    "!pip install torchvision\n",
    "!pip install torchgeometry\n",
    "!conda install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "book_text =  get_book_text('798-8')\n",
    "ner_model = 'mrm8488/mobilebert-finetuned-ner'\n",
    "tokenizer = AutoTokenizer.from_pretrained(ner_model)\n",
    "model = AutoModelForTokenClassification.from_pretrained(ner_model)\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "sentence_level_book = re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', book_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new entity tokens\n",
      "[{'entity': 'I-PER', 'score': 0.9980604, 'index': 42, 'word': 'm', 'start': 113, 'end': 114}, {'entity': 'I-PER', 'score': 0.9974752, 'index': 44, 'word': 'le', 'start': 116, 'end': 118}, {'entity': 'I-PER', 'score': 0.93767214, 'index': 45, 'word': 'mai', 'start': 119, 'end': 122}, {'entity': 'I-PER', 'score': 0.906227, 'index': 46, 'word': '##re', 'start': 122, 'end': 124}, {'entity': 'I-PER', 'score': 0.9871251, 'index': 48, 'word': 'jacob', 'start': 126, 'end': 131}, {'entity': 'I-PER', 'score': 0.990449, 'index': 49, 'word': '##in', 'start': 131, 'end': 133}, {'entity': 'I-PER', 'score': 0.955765, 'index': 51, 'word': 'bon', 'start': 137, 'end': 140}, {'entity': 'I-PER', 'score': 0.8932861, 'index': 54, 'word': '##ste', 'start': 146, 'end': 149}, {'entity': 'I-PER', 'score': 0.9953166, 'index': 97, 'word': 'm', 'start': 262, 'end': 263}, {'entity': 'I-PER', 'score': 0.9966055, 'index': 99, 'word': 'de', 'start': 265, 'end': 267}, {'entity': 'I-PER', 'score': 0.9961195, 'index': 100, 'word': 'renal', 'start': 268, 'end': 273}]\n",
      "line words\n",
      "['Un', 'vieux', 'chirurgien', '-', 'major', 'de', 'l', \"'\", 'armée', 'd', \"'\", 'Italie', ',', 'retiré', 'à', 'Verrières', ',', 'et', 'qui', 'de', 'son', 'vivant', 'était', 'à', 'la', 'fois', ',', 'suivant', 'M', '.', 'le', 'maire', ',', 'jacobin', 'et', 'bonapartiste', ',', 'osa', 'bien', 'un', 'jour', 'se', 'plaindre', 'à', 'lui', 'de', 'la', 'mutilation', 'périodique', 'de', 'ces', 'beaux', 'arbres', '.', '-', '-', 'J', \"'\", 'aime', 'l', \"'\", 'ombre', ',', 'répondit', 'M', '.', 'de', 'Rênal', 'avec', 'la', 'nuance', 'de', 'hauteur', 'convenable', 'quand', 'on', 'parle', 'à', 'un', 'chirurgien', ',', 'membre', 'de', 'la', 'Légion', 'd', \"'\", 'honneur', ',', 'j', \"'\", 'aime', 'l', \"'\", 'ombre', ',', 'je', 'fais', 'tailler', 'mes', 'arbres', 'pour', 'donner', 'de', 'l', \"'\", 'ombre', ',', 'et', 'je', 'ne', 'conçois', 'pas', 'qu', \"'\", 'un', 'arbre', 'soit', 'fait', 'pour', 'autre', 'chose', ',', 'quand', 'toutefois', ',', 'comme', 'l', \"'\", 'utile', 'noyer', ',', 'il', '_', 'ne', 'rapporte', 'pas', 'de', 'revenu', '_', '.']\n"
     ]
    }
   ],
   "source": [
    "l=sentence_level_book[65]\n",
    "a,b,c = get_line_entities(l, [], [], 0, tokenizer, nlp,\n",
    "                     False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': '▁d',\n",
       "  'score': 0.5566989183425903,\n",
       "  'entity': 'I-LOC',\n",
       "  'index': 10,\n",
       "  'start': 36,\n",
       "  'end': 38},\n",
       " {'word': 'Italie',\n",
       "  'score': 0.9557802677154541,\n",
       "  'entity': 'I-LOC',\n",
       "  'index': 12,\n",
       "  'start': 39,\n",
       "  'end': 45},\n",
       " {'word': '▁V',\n",
       "  'score': 0.998179018497467,\n",
       "  'entity': 'I-LOC',\n",
       "  'index': 16,\n",
       "  'start': 55,\n",
       "  'end': 57},\n",
       " {'word': 'errière',\n",
       "  'score': 0.9979278445243835,\n",
       "  'entity': 'I-LOC',\n",
       "  'index': 17,\n",
       "  'start': 57,\n",
       "  'end': 64},\n",
       " {'word': 's',\n",
       "  'score': 0.9982990026473999,\n",
       "  'entity': 'I-LOC',\n",
       "  'index': 18,\n",
       "  'start': 64,\n",
       "  'end': 65},\n",
       " {'word': '▁M',\n",
       "  'score': 0.9855217337608337,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 31,\n",
       "  'start': 112,\n",
       "  'end': 114},\n",
       " {'word': '.',\n",
       "  'score': 0.9845024347305298,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 32,\n",
       "  'start': 114,\n",
       "  'end': 115},\n",
       " {'word': '▁le',\n",
       "  'score': 0.8523029685020447,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 33,\n",
       "  'start': 115,\n",
       "  'end': 118},\n",
       " {'word': '▁maire',\n",
       "  'score': 0.8729842901229858,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 34,\n",
       "  'start': 118,\n",
       "  'end': 124},\n",
       " {'word': '▁M',\n",
       "  'score': 0.9234569668769836,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 74,\n",
       "  'start': 261,\n",
       "  'end': 263},\n",
       " {'word': '.',\n",
       "  'score': 0.9141836166381836,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 75,\n",
       "  'start': 263,\n",
       "  'end': 264},\n",
       " {'word': '▁de',\n",
       "  'score': 0.9488356113433838,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 76,\n",
       "  'start': 264,\n",
       "  'end': 267},\n",
       " {'word': '▁R',\n",
       "  'score': 0.992038905620575,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 77,\n",
       "  'start': 267,\n",
       "  'end': 269},\n",
       " {'word': 'ê',\n",
       "  'score': 0.9914994239807129,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 78,\n",
       "  'start': 269,\n",
       "  'end': 270},\n",
       " {'word': 'nal',\n",
       "  'score': 0.991144597530365,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 79,\n",
       "  'start': 270,\n",
       "  'end': 273},\n",
       " {'word': '▁Légion',\n",
       "  'score': 0.9943797588348389,\n",
       "  'entity': 'I-ORG',\n",
       "  'index': 96,\n",
       "  'start': 355,\n",
       "  'end': 362},\n",
       " {'word': '▁d',\n",
       "  'score': 0.9943034648895264,\n",
       "  'entity': 'I-ORG',\n",
       "  'index': 97,\n",
       "  'start': 362,\n",
       "  'end': 364},\n",
       " {'word': 'honneur',\n",
       "  'score': 0.9942684173583984,\n",
       "  'entity': 'I-ORG',\n",
       "  'index': 99,\n",
       "  'start': 365,\n",
       "  'end': 372}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7929/7929 [09:56<00:00, 13.28it/s]\n"
     ]
    }
   ],
   "source": [
    "rouge_noir_id = '798-8'\n",
    "(rouge_noir_text, \n",
    " rouge_noir_ent_tokens, \n",
    " rouge_noir_ent_words) = get_person_entities(rouge_noir_id, grouped_entities=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_word\n",
       "julien        1166\n",
       "rênal          595\n",
       "mathilde       340\n",
       "mole           170\n",
       "pirard         106\n",
       "abbé            81\n",
       "croisenois      53\n",
       "sorel           53\n",
       "derville        50\n",
       "norbert         49\n",
       "fervaques       38\n",
       "élisa           29\n",
       "valenod         28\n",
       "napoléon        27\n",
       "altamira        27\n",
       "caylus          27\n",
       "louis           24\n",
       "trouva          23\n",
       "chélan          21\n",
       "amanda          19\n",
       "geronimo        17\n",
       "frilair         16\n",
       "stanislas       16\n",
       "maslon          14\n",
       "don             14\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save entities into a dataframe, and to the disk\n",
    "rouge_noir_df = pd.DataFrame(rouge_noir_ent_words)\n",
    "# view top 25 entities\n",
    "(rouge_noir_df\n",
    " .drop_duplicates('total_word_index')\n",
    " .groupby('full_word')\n",
    " .count()\n",
    " .sort_values(by='score', ascending=False)\n",
    ")['score'][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save entities into a dataframe, and to the disk\n",
    "rouge_noir_df = pd.DataFrame(rouge_noir_ent_words)\n",
    "# rouge_noir_df['full_word'] =  rouge_noir_df['full_word'].apply(lambda s: s.lower())\n",
    "rouge_noir_df.to_csv('../data/book_dfs/rouge_noir_df.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_word\n",
       "julien        450\n",
       "é             369\n",
       "mme           208\n",
       "mathilde       98\n",
       "è              75\n",
       "valenod        70\n",
       "mlle           63\n",
       "mole           54\n",
       "ê              48\n",
       "marquis        43\n",
       "ch             32\n",
       "tait           31\n",
       "sorel          29\n",
       "re             24\n",
       "fouqu          24\n",
       "disait         21\n",
       "tre            19\n",
       "comte          19\n",
       "pr             18\n",
       "ç              17\n",
       "grand          16\n",
       "norbert        16\n",
       "derville       15\n",
       "û              14\n",
       "croisenois     14\n",
       "pensa          14\n",
       "pirard         14\n",
       "res            13\n",
       "chevalier      11\n",
       "nal            11\n",
       "homme          11\n",
       "monsieur       11\n",
       "jour           10\n",
       "mar            10\n",
       "chambre        10\n",
       "napol           9\n",
       "saint           9\n",
       "abb             9\n",
       "tanbeau         8\n",
       "dieu            8\n",
       "esprit          8\n",
       "appert          8\n",
       "maire           7\n",
       "ami             7\n",
       "marquise        7\n",
       "duc             7\n",
       "amour           7\n",
       "caract          7\n",
       "altamira        7\n",
       "voir            7\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view top 25 entities\n",
    "(rouge_noir_df\n",
    " .drop_duplicates('total_word_index')\n",
    " .groupby('full_word')\n",
    " .count()\n",
    " .sort_values(by='score', ascending=False)\n",
    ")['score'][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_word\n",
       "julien        1166\n",
       "rênal          595\n",
       "mathilde       340\n",
       "mole           170\n",
       "pirard         106\n",
       "abbé            81\n",
       "croisenois      53\n",
       "sorel           53\n",
       "derville        50\n",
       "norbert         49\n",
       "fervaques       38\n",
       "élisa           29\n",
       "valenod         28\n",
       "napoléon        27\n",
       "altamira        27\n",
       "caylus          27\n",
       "louis           24\n",
       "trouva          23\n",
       "chélan          21\n",
       "amanda          19\n",
       "geronimo        17\n",
       "frilair         16\n",
       "stanislas       16\n",
       "maslon          14\n",
       "don             14\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save entities into a dataframe, and to the disk\n",
    "rouge_noir_df = pd.DataFrame(rouge_noir_ent_words)\n",
    "# rouge_noir_df['full_word'] =  rouge_noir_df['full_word'].apply(lambda s: s.lower())\n",
    "rouge_noir_df.to_csv('../data/book_dfs/798-8.csv', index=False) \n",
    "\n",
    "# view top 25 entities\n",
    "(rouge_noir_df\n",
    " .drop_duplicates('total_word_index')\n",
    " .groupby('full_word')\n",
    " .count()\n",
    " .sort_values(by='score', ascending=False)\n",
    ")['score'][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-65cd5ac06478>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../src'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcharacter_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/students/doyard/sentiment-evolution-in-characters-network/src/character_extraction.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../src')\n",
    "\n",
    "from character_extraction import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_text =  get_book_text('798-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_pg_id = '798-8'\n",
    "book_luke_checkpoints_tmp_dir = f'../data/book_dfs/tmp/{book_pg_id}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7929 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "continue_from_ckpt = False\n",
    "\n",
    "if not continue_from_ckpt:\n",
    "    book_text, ner_entities_words = get_LUKE_person_entities(book_pg_id, \n",
    "                                                             book_luke_checkpoints_tmp_dir, \n",
    "                                                             ner_entities_words = [])\n",
    "    \n",
    "else:\n",
    "    # continue from checkpoint, if needed\n",
    "    latest_checkpoint = 10500\n",
    "    luke_df = pd.read_csv(f'{book_luke_checkpoints_tmp_dir}luke_tmp_df_{latest_checkpoint:05d}.csv')\n",
    "    ner_entities_checkpoint = [dict(zip(['full_word', 'sentence_word_index', 'total_word_index'], v)) \n",
    "                               for v in [tuple(r) for r in luke_df.to_numpy()]]\n",
    "\n",
    "    book_text, ner_entities_words = get_LUKE_person_entities(book_pg_id, \n",
    "                                                             book_luke_checkpoints_tmp_dir,\n",
    "                                                             last_checkpoint = latest_checkpoint,\n",
    "                                                             ner_entities_words = ner_entities_checkpoint)\n",
    "    \n",
    "luke_df = pd.DataFrame(ner_entities_words)\n",
    "luke_df.to_csv(f'notebooks_data/book_dfs/luke_{book_pg_id}_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7929/7929 [10:07<00:00, 13.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "full_word\n",
       "julien        1166\n",
       "rênal          595\n",
       "mathilde       340\n",
       "mole           170\n",
       "pirard         106\n",
       "abbé            81\n",
       "croisenois      53\n",
       "sorel           53\n",
       "derville        50\n",
       "norbert         49\n",
       "fervaques       38\n",
       "élisa           29\n",
       "valenod         28\n",
       "napoléon        27\n",
       "altamira        27\n",
       "caylus          27\n",
       "louis           24\n",
       "trouva          23\n",
       "chélan          21\n",
       "amanda          19\n",
       "geronimo        17\n",
       "frilair         16\n",
       "stanislas       16\n",
       "maslon          14\n",
       "don             14\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_noir_id = '798-8'\n",
    "(rouge_noir_text, \n",
    " rouge_noir_ent_tokens, \n",
    " rouge_noir_ent_words) = get_person_entities(rouge_noir_id, grouped_entities=False)\n",
    "\n",
    "# save entities into a dataframe, and to the disk\n",
    "red_black_df = pd.DataFrame(rouge_noir_ent_words)\n",
    "# rouge_noir_df['full_word'] =  rouge_noir_df['full_word'].apply(lambda s: s.lower())\n",
    "red_black_df.to_csv('../data/book_dfs/red_black_df.csv', index=False) \n",
    "\n",
    "# view top 25 entities\n",
    "(red_black_df\n",
    " .drop_duplicates('total_word_index')\n",
    " .groupby('full_word')\n",
    " .count()\n",
    " .sort_values(by='score', ascending=False)\n",
    ")['score'][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from character_extraction import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/7929 [00:00<10:21, 12.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8c61ea6aa29b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m (rouge_noir_text, \n\u001b[1;32m      3\u001b[0m  \u001b[0mrouge_noir_ent_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m  rouge_noir_ent_words) = get_person_entities(rouge_noir_id, grouped_entities=True)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# save entities into a dataframe, and to the disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/MA-4/Projet de Semestre/github/sentiment-evolution-in-characters-network/src/character_extraction.py\u001b[0m in \u001b[0;36mget_person_entities\u001b[0;34m(gutenberg_id, grouped_entities, max_chunk_len, split_chunk_len)\u001b[0m\n\u001b[1;32m    179\u001b[0m                                                                                             \u001b[0mner_entities_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                                                                                             \u001b[0mner_entities_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                                                                                             \u001b[0msentence_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                                                                                             \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                                                                                             \u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/MA-4/Projet de Semestre/github/sentiment-evolution-in-characters-network/src/character_extraction.py\u001b[0m in \u001b[0;36mget_line_entities\u001b[0;34m(l, ner_entities_tokens, ner_entities_words, sentence_index, tokenizer, nlp, grouped_entities)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mnew_entity_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgrouped_entities\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mnew_entity_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'PER'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entity_group'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mnew_entity_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'PER'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/pipelines/token_classification.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"offset_mapping\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffset_mapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset_mapping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    941\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/pipelines/token_classification.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         return {\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1560\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1562\u001b[0;31m         outputs = self.mobilebert(\n\u001b[0m\u001b[1;32m   1563\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_hidden_states, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         )\n\u001b[0;32m--> 874\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    875\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    553\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    556\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         outputs = (\n\u001b[1;32m    519\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, intermediate_states, residual_tensor_1, residual_tensor_2)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresidual_tensor_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbottleneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual_tensor_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, residual_tensor)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_outputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresidual_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rouge_noir_id = '798-8'\n",
    "(rouge_noir_text, \n",
    " rouge_noir_ent_tokens, \n",
    " rouge_noir_ent_words) = get_person_entities(rouge_noir_id, grouped_entities=True)\n",
    "\n",
    "# save entities into a dataframe, and to the disk\n",
    "rouge_noir_df = pd.DataFrame(rouge_noir_ent_words)\n",
    "# rouge_noir_df['full_word'] =  rouge_noir_df['full_word'].apply(lambda s: s.lower())\n",
    "rouge_noir_df.to_csv('../data/book_dfs/798-8.csv', index=False) \n",
    "\n",
    "# view top 25 entities\n",
    "(rouge_noir_df\n",
    " .drop_duplicates('total_word_index')\n",
    " .groupby('full_word')\n",
    " .count()\n",
    " .sort_values(by='score', ascending=False)\n",
    ")['score'][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
